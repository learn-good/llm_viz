<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Decoder-only transformer</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #1e1e1e;
            color: #e0e0e0;
        }

        #reporting-container {
            font-size: 16px;
            line-height: 1.5;
            font-family: 'Courier New', Courier, monospace;
            padding: 20px;
            width: 400px;
            /* background-color: #2a2a2a; */
        }

        #main-container {
            display: flex;
            width: 100%;
            height: 800px;
        }

        #svg-container {
            flex: 0 0 1200px;
            z-index: 10;
        }

        #svg {
            cursor: move;
        }


        #right-container {
            flex: 1;
            display: flex;
            flex-direction: column;
            height: 840px;
        }

        .container {
            display: flex;
            justify-content: center;
        }

        #info-container {
            flex: 4;
            display: flex;
            flex-direction: column;
            padding: 10px;
        }

        #info {
            /* margin-bottom: 20px; */
            /* height: 550px; */
            flex: 1;
            padding: 10px;
            border: 1px solid #444;
            font-size: 16px;
            line-height: 1.5;
            border-radius: 15px;
            overflow: hidden;
            background-color: #2a2a2a;
        }

        #info .katex {
            font-size: 1.2em;
        }

        .nav-button {
            cursor: pointer;
        }

        .clickable {
            cursor: pointer;
        }

        /* Dark mode specific styles */
        svg {
            background-color: #2a2a2a;
            border-radius: 15px;
            padding: 10px;
            margin: 10px 0 10px 10px;
            border: 1px solid #444;
        }

        .latex-label:hover {
            cursor: default;
        }

        .latex-label[data-link]:hover {
            cursor: pointer;
        }

        /* References */
        #references-container {
            /* height: 250px; */
            flex: 1;
            display: flex;
            flex-direction: row;
            padding: 0px 10px 10px 10px;
        }

        #references {
            /* height: 220px; */
            flex: 1;
            padding: 10px;
            background-color: #2a2a2a;
            border: 1px solid #444;
            border-radius: 15px;
        }

        #references-container h3 {
            margin-top: 0;
            color: #b4b4b4;
        }

        #references-list {
            height: 150px;
            list-style-type: none;
            padding: 5px;
            overflow-y: auto;
            margin: 0;
        }

        #references-list li {
            margin-bottom: 10px;
        }

        #references-list a {
            color: #b4b4b4;
            text-decoration: none;
        }

        #references-list a:hover {
            color: #00ffff;
            text-decoration: underline;
        }


        /* text {
            fill: #e0e0e0;
        } */
    </style>
    <!-- for LaTeX -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js"></script>
    <!-- for plotting data -->
    <script src="https://d3js.org/d3.v7.min.js"></script>
</head>

<body>
    <!--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
        HTML
     -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-->
    <div id="main-container">
        <div id="svg-container">
            <svg id="svg" width="1200" height="800">
                <g id="content-layer"></g>
                <g id="navigation-layer"></g>
            </svg>
        </div>
        <div id="right-container">
            <div id="info-container">
                <div id="info"></div>

            </div>
            <div id="references-container">
                <div id="references">
                    <h3>References</h3>
                    <ul id="references-list"></ul>
                </div>
            </div>
        </div>
    </div>
    <div id="reporting-container">
        <div id="log-container">
        </div>
    </div>

    <!--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
        Javascript
     -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-->
    <script>
        // Globals
        let zoom;
        let contentGroup;


        // SVG Navigation
        let currentView = 'root';
        let undoNavigationHistory = [];
        let redoNavigationHistory = [];

        // SVG paths for navigation arrows
        const arrowPaths = {
            up: "M5,20 L15,5 L25,20 Z",
            back: "M20,5 L5,15 L20,25 Z",
            forward: "M10,5 L25,15 L10,25 Z",
            reset: "M15,5 A10,10 0 1,0 15,25 A10,10 0 1,0 15,5 "
        };
        const repeatPath = "M15,5 A10,10 0 1,0 25,15 M25,10 L21,16 M25,10 L29,16";
        const positionalEncodingPath = "M15,5 A10,10 0 1,0 15,25 A10,10 0 1,0 15,5 M5,15 Q10,8 15,15 Q20,22 25,15";

        // Color constants
        const BACKGROUND_COLOR = '#2a2a2a';
        const RECT_FILL_COLOR = '#3a3a3a';
        const RECT_STROKE_COLOR = '#555';
        const TEXT_COLOR = '#e0e0e0';
        const BUTTON_FILL_COLOR = '#3a3a3a';
        const BUTTON_STROKE_COLOR = '#555';
        const BUTTON_ARROW_COLOR = '#e0e0e0';
        const BUTTON_HOVER_FILL_COLOR = '#4a4a4a';
        const BUTTON_HOVER_ARROW_COLOR = '#ffffff';
        const DISABLED_BUTTON_FILL_COLOR = '#2a2a2a';
        const DISABLED_BUTTON_STROKE_COLOR = '#3a3a3a';
        const DISABLED_BUTTON_ARROW_COLOR = '#4a4a4a';
        const LATEX_LINK_HOVER_COLOR = "#00ffff";
        const LINK_COLOR = '#00ffff';
        const LATEX_LABEL_COLOR = "#ffffff";
        const ARROW_COLOR = "#999";
        const POS_ENC_COLOR = "#333";


        const LATEX_FONT_SIZE = 18;

        const references = [
            {
                title: "Attention Is All You Need",
                authors: [
                    "Ashish Vaswani",
                    "Noam Shazeer",
                    "Niki Parmari",
                    "Jakob Uszkoreit",
                    "Llion Jones",
                    "Aidan N. Gomez",
                    "Lukasz Kaiser",
                    "Illia Polosukhin",
                ],
                link: "https://arxiv.org/abs/1706.03762",
                info: "The original paper introducing the Transformer architecture.",
                refType: "arXiv"
            },
            {
                title: "nanoGPT",
                authors: ["Andrej Karpathy", "Github contributors"],
                link: "https://github.com/karpathy/nanoGPT/blob/master/model.py",
                info: "A minimal implementation of GPT in PyTorch.",
                refType: "Github"
            },
            {
                title: "llm.c",
                authors: ["Andrej Karpathy", "Github contributors"],
                link: "https://github.com/karpathy/llm.c",
                info: "TODO",
                refType: "Github"
            },
            {
                title: "\"Let's build GPT: from scratch, in code, spelled out\"",
                authors: ["Andrej Karpathy"],
                link: "https://www.youtube.com/watch?v=kCc8FmEb1nY",
                info: "A detailed video tutorial on building a GPT model from scratch.",
                refType: "YouTube"
            },
            {
                title: "\"Let's reproduce GPT-2\"",
                authors: ["Andrej Karpathy"],
                link: "https://www.youtube.com/watch?v=l8pRSuU81PU",
                info: "A video tutorial on reproducing the GPT-2 (124M) model.",
                refType: "YouTube"
            },
            {
                title: "\"Let's build the GPT Tokenizer\"",
                authors: ["Andrej Karpathy"],
                link: "https://www.youtube.com/watch?v=zduSFxRajkE",
                info: "A detailed video tutorial on tokenization in GPT models",
                refType: "YouTube"
            },
            {
                title: "Tiktoken",
                authors: ["OpenAI", "Shantanu Jain", "Github Contributors"],
                link: "https://github.com/openai/tiktoken",
                info: "A fast BPE tokeniser for use with OpenAI's models.",
                refType: "Github"
            },
            {
                title: "Tiktokenizer App",
                authors: ["David Duong"],
                link: "https://tiktokenizer.vercel.app/",
                info: "An interactive application to tokenize text using different tokenizers",
                refType: "web app"
            },
            {
                title: "Using the Output Embedding to Improve Language Models",
                authors: ["Ofir Press", "Lior Wolf"],
                link: "https://arxiv.org/abs/1608.05859",
                info: "A paper discussing techniques to improve language models by tying the token input embedding and the self-attention output embedding",
                refType: "arXiv"
            },
            {
                title: "The Llama 3 Herd of Models",
                authors: ["The Llama team at Meta"],
                link: "https://arxiv.org/abs/2407.21783",
                info: "A paper describing the Llama 3 family of large language models.",
                refType: "arXiv"
            },
            {
                title: "llama-models",
                authors: ["Meta", "Github contributors"],
                link: "https://github.com/meta-llama/llama-models",
                info: "Contains reference implementations and information about the Llama family of models.",
                refType: "arXiv"
            },
            {
                title: "RoFormer: Enhanced Transformer with Rotary Position Embedding",
                authors: [
                    "Jianlin Su",
                    "Yu Lu",
                    "Shengfeng Pan",
                    "Ahmed Murtadha",
                    "Bo Wen",
                    "Yunfeng Liu"
                ],
                link: "https://arxiv.org/abs/2104.09864",
                info: "A paper introducing Rotary Position Embedding, an alternative to traditional positional encodings in transformer models.",
                refType: "arXiv"
            },
            {
                title: "Rotary Positional Embeddings: Combining Absolute and Relative",
                authors: [
                    "Bai Li"
                ],
                link: "https://www.youtube.com/watch?v=o29P0Kpobz0",
                info: "A video explanation of RoPE - Rotary Positional Embeddings",
                refType: "YouTube"
            },
            {
                title: "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
                authors: [
                    "Nitish Srivastava",
                    "Geoffrey Hinton",
                    "Alex Krizhevsky",
                    "Ilya Sutskever",
                    "Ruslan Salakhutdinov"
                ],
                link: "https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf",
                info: "This seminal paper introduces the dropout technique, a simple yet effective method to reduce overfitting in neural networks by randomly 'dropping out' a proportion of neurons during training.",
                refType: "PDF"
            },
            {
                title: "Online normalizer calculation for softmax",
                authors: [
                    "Maxim Milakov",
                    "Natalia Gimelshein",
                ],
                link: "https://arxiv.org/abs/1805.02867",
                info: "Proposes an online, memory-access efficient algorithm for calculating the numerically stable softmax.",
                refType: "arXiv"
            },
            {
                title: "Deep Residual Learning for Image Recognition",
                authors: [
                    "Kaiming He",
                    "Xiangyu Zhang",
                    "Shaoqing Ren",
                    "Jian Sun"
                ],
                link: "https://arxiv.org/abs/1512.03385",
                info: "Introduces residual connections for use in deep neural networks, allowing for easier optimization of very deep networks.",
                refType: "arXiv"
            },
            {
                title: "Layer Normalization",
                authors: [
                    "Jimmy Lei Ba",
                    "Jamie Ryan Kiros",
                    "Geoffrey E. Hinton"
                ],
                link: "https://arxiv.org/abs/1607.06450",
                info: "This paper introduces Layer Normalization, a technique to normalize the inputs across the features, which helps in reducing the covariate shift problem in deep neural networks.",
                refType: "arXiv"
            },
            {
                title: "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
                authors: [
                    "Tri Dao",
                    "Daniel Y. Fu",
                    "Stefano Ermon",
                    "Atri Rudra",
                    "Christopher Ré"
                ],
                link: "https://arxiv.org/abs/2205.14135",
                info: "This paper presents FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce memory access and increase efficiency in transformer models.",
                refType: "arXiv"
            },
        ]

        //-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
        // SVG Definition
        //-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
        const svgWidth = 1200;
        const svgHeight = 800;
        const svg = d3.select("#svg")
            .attr("width", svgWidth)
            .attr("height", svgHeight)
            .style("background-color", BACKGROUND_COLOR);

        const contentLayer = d3.select("#content-layer");
        const navigationLayer = d3.select("#navigation-layer");
        // const hiddenContent = d3.select("#hidden-content");

        const SINGLE_LATEX_CHAR_HEIGHT = 50;
        const SINGLE_LATEX_CHAR_WIDTH = 50;
        const SINGLE_VECTOR_WIDTH = 24;
        const SINGLE_VECTOR_HEIGHT = 400;
        const MIN_SEP_HORIZONTAL = 200;

        // Rule for whether something can have a link:
        //    try to explain in allotted space with text
        //    if after that visual help still useful, make a link
        const x_raw_info = `$$x_{raw}$$ represents the raw input data for the transformer model.

**During training**: The input is usually a large corpus of text data, such as books, articles, or web pages. This can be several terabytes of data. It gets tokenized and split into manageable sequences (e.g., 1024 tokens) for batch processing.

**During inference**: The input is typically a prompt or partial sequence provided by the user. This can range from a few words to several paragraphs, depending on the task. This input is then tokenized and the model then generates a continuation based on this input.

The input can include various languages, special characters, and formatting, which the subsequent tokenization step will process.

For non-text data types, appropriate preprocessing steps would be applied before feeding the input into the transformer architecture.`
        const x_raw_link = "" // "x_raw"
        const x_tok_info = `The tokenized input $$x_{tok}$$ serves as the starting point for the transformer's processing, capturing the structure and content of the input text in a format that is usable by the neural network.

**About tokenized data:**
1. **Vocabulary size**: Tokens are drawn from a fixed vocabulary, e.g. \`vocab_size\` $$ = d_{vocab} = 64128$$.

2. **Special tokens**: These include tokens like \`[START]\`, \`[END]\`, \`[PAD]\`, \`[UNK]\` for start of sequence, end of sequence, padding, and unknown words respectively.

3. **Subword tokenization**: Most modern tokenizers (e.g., BPE, WordPiece, SentencePiece) use subword units.

4. **Sequence length**: Tokenized sequences are typically padded or truncated to a fixed length (e.g., 512 or 4096 tokens). We will refer to this number as $$d_{seq}$$.

5. **Token IDs**: Each token is represented by a unique integer ID, which is used as input to the embedding layer.`
        const x_tok_link = "" //"x_tok"
        const x_embd_info = `$$x_{embd}$$ represents the embedded data that serves a the enriched input data for the transformer model.

**About embedded input:**
1. **Shape**: $$x_{embd}$$ has shape $$(d_{seq} \\times d_{model})$$, where $$d_{seq}$$ is the sequence length and $$d_{model}$$ is the embedding dimension.

2. **Learned token embeddings**: The embedding matrix learns richer representations of tokens than just the token ID present in $$x_{tok}$$, capturing semantic meaning about the input tokens.

3. **Input to attention layers**: $$x_{embd} + PE$$ serves as the initial input to the self-attention mechanism in the transformer, where $$PE$$ is the positional encoding used to capture information about a token's position in the sequence.

4. **Why do we need to add positional encoding?**: The self-attention mechanism in transformers is permutation-invariant, meaning it treats the input as a set rather than a sequence. There are many approaches to address this limitation, such as absolute (pictured), relative, and rotary position embeddings.`;
        const x_embd_link = "" // "x_embd"
        const tokenization_info = `Tokenization bridges the gap between human-readable text and the numerical input required by transformer models. The tokenizer converts raw text into a sequence of integers, or token IDs, which the model can then process. The tokenizer is constructed separately from the rest of the transformer, potentially using different data.

**BPE** (Byte Pair Encoding) is a popular tokenization method that starts with individual characters and iteratively merges the most frequent pairs to form new tokens, while other approaches like WordPiece and SentencePiece offer alternative strategies.

**Some of the potential issues with tokenization**:
1. **Out-of-vocabulary words**: Rare or unseen words may be split into suboptimal subwords.

2. **Contextual meaning loss**: Splitting words into subwords and inadvertant groupings can sometimes cause loss of important context.

3. **Inconsistent tokenization**: The same word might be tokenized differently depending on the characters around it.`
        const tokenization_link = "" // "tokenization"
        const token_embd_info = `Token embedding converts tokenized input into dense vector representations, allowing for the model to learn a richer representation for each token.

**Key aspects of token embedding:**
1. **Lookup table**: Each token is mapped to a unique vector in a high-dimensional space, (e.g., $$d_{model}=512$$, where $$d_{model}$$ is commonly used to refer to the embedding dimension). When $$x_{tok}$$ is one-hot encoded, it is expanded such that the index pertaining the the token ID is set to $$1,$$ and all others are set to $$0,$$ which leads to that index \"looking up\" its higher dimensional representation via matrix multiplication.

2. **Learned representations**: These embeddings are learned during the training process, capturing semantic and syntactic information about tokens.

3. **Weight tying**: The embedding matrix is often shared with the final linear projection layer in the model's output, reducing the number of parameters and improving generalization. The intuition is that the process of generating a word (output) should be closely related to the process of understanding a word (input).

The embedding process can be represented as: 
$$x_{embd} = x_{tok} \\cdot W_{embd} $$

Where $$x_{tok}$$ is one-hot encoded from shape $$( d_{seq} \\times 1 )$$ to $$( d_{seq} \\times d_{vocab} )$$, and $$W_{embd}$$ is the embedding matrix of shape: $$( d_{vocab} \\times d_{model} )$$. The result $$x_{embd}$$ is shape $$( d_{seq} \\times d_{model} )$$.`;

        const token_embd_link = "" //"token_embedding"
        const pos_enc_info = `The self-attention mechanism in transformers is permutation-invariant, meaning it doesn't inherently consider the order of tokens. $$PE$$ adds information about the position of each token in the sequence.

        The original sinusoidal $$PE$$ (Vaswani et al., 2017) uses sine and cosine functions of different frequencies to create unique encodings for each position. This is one version of an absolute position encoding, where the encoding is based solely on the absolute position of the input.

**Alternative approaches:**
1. **Learned PE**: Trainable embeddings for each position.

2. **Relative PE**: Encodes relative distances between tokens instead of absolute positions.

3. **Rotary PE (RoPE)**: Applies rotation to query vector $$Q$$ and key matrix $$K$$ that are generated from the token embeddings.

The choice of PE can significantly impact model performance and generalization ability, especially for tasks sensitive to token order or involving extrapolation to longer sequences.`;
        const pos_enc_link = "" // "position_encoding"
        const attn_info = "todo";
        const attn_link = "attention"
        // const est_prob_dist_i = "position_encoding"

        const logits_info = `Logits are the raw, unnormalized predictions output by the transformer model before the final softmax layer.

**Key points about logits:**
1. **Definition**: Logits are the direct output of the final linear layer in the transformer, representing the model's raw predictions for each token in the vocabulary.

2. **Shape**: The logits vector has length of $$d_{vocab}$$, where $$d_{vocab}$$ is the vocabulary size.

3. **Interpretation**: Each value in the logits vector represents the model's unnormalized confidence for a particular token or class.

4. **Temperature**: Before converting to probabilities, logits are often divided by a temperature parameter to control the randomness of the output distribution.

The logits represent the model's raw decision space before it's constrained to a probability distribution, offering a view into the model's pre-normalized predictions.`;

        const dropout_info = `Dropout is a regularization technique used to prevent overfitting in neural networks, including transformers.

**Key points about dropout:**
1. **Mechanism**: During training, randomly "drops out" (sets to zero) a proportion of neurons.

2. **Purpose**: Reduces co-adaptation between neurons, forcing the network to learn more robust features.

3. **Original paper**: The "Attention Is All You Need" paper used dropout in several places, including after the attention and feedforward layers, and in the embedding layers.

**Important note**: While dropout was used in the original transformer and is common in many models, it's not always necessary. For example, the Llama 3 family of models doesn't use dropout at all, demonstrating that high-performing models can be built without this technique.`;
        const lin_prj_info = `The linear projection layer serves as the final transformation to recover the vocab size dimension, $$d_{vocab}$$, before the softmax operation to get next token probabilities.

**About linear projection:**
1. **Purpose**: It maps the output of the self-attention blocks to logits over the vocabulary size.

2. **Operation**: It's a simple matrix multiplication.

3. **Weight tying**: In many implementations, $$W_{proj}$$ is often tied (shared) with the input embedding matrix, $$W_{embd}$$, which can improve performance and reduce the number of parameters.

The linear projection prepares the model's raw predictions (logits) for the final probability distribution over the vocabulary, which is then computed by the softmax function.`;

        const softmax_info = `The softmax function is applied to the logits to produce a probability distribution over the vocabulary for the next token.

**About softmax:**
1. **Formula and Temperature**: For input vector $$z$$, the softmax function is defined as:
   $$\\text{softmax}(z_i, T) = \\dfrac{e^{z_i/T}}{\\sum_{j=1}^K e^{z_j/T}}$$

   where $$K$$ is the number of classes (vocabulary size $$d_{vocab}$$ in this case). A temperature parameter $$T$$ can be applied to control the "sharpness" of the distribution.
    - Lower T (< 1) makes distribution more peaked (less random)
    - Higher T (> 1) makes distribution more uniform (more random)

2. **Properties**: Output values are between $$0$$ and $$1$$; sum of all outputs equals $$1$$; preserves relative ordering of inputs

3. **Numerical Stability and Optimization**: Softmax is often implemented with a "max trick" to prevent overflow. There is also an \"online\" version that allows for carrying out the computation with fewer memory accesses, resulting in faster performance (Milakov et al., 2018).`;

        const pos_encoding_arrows_info = "Apply positional encoding to embedded input $$x_{embd}$$ via addition (only if using the original positional encoding schema, else add $$0$$ to $$x_{embd}$$)";
        const addition_info = "Addition, if using the original positional encoding proposed but the original \"Attention is All You Need\" paper. Otherwise you can think of this as just a pass through connection for $$x_{embd}$$, or addition with $$0$$ if you'd prefer."

        const est_prob_dist_info = `The estimated probability distribution $$\\hat{P}( x_{tok}^{(t+1)} | x_{tok}^{(...t)} )$$ represents the model's predicted probability distribution for the next token given the previous tokens in the sequence.

**Key points:**
1. **Shape**: A vector of length $$d_{vocab}$$, where each element corresponds to a token in the vocabulary.

2. **Interpretation**: The value at vector index $$i$$ represents the model's estimated probability that the $$i$$-th token in the vocabulary will be the next token in the sequence.

3. **Sampling**: During text generation, the next token is typically sampled from this distribution, allowing for controlled randomness in the output.`;

        const sampling_info = `Sampling is the process of selecting the next token based on the probability distribution output by the model.

**Examples of sampling:**
1. **Top-k sampling**: Only consider the $$k$$ most likely next tokens.
   - Example: If $$k = 5$$, only the $$5$$ tokens with the highest probabilities are considered.
   - Process: Sort tokens by probability; Keep only the top k tokens; Renormalize probabilities; Sample from this reduced set

2. **Top-p (nucleus) sampling**: Consider the smallest set of tokens whose cumulative probability exceeds the probability p.
   - Example: If $$p = 0.9$$, select the smallest set of tokens that sum to 90% probability.
   - Process: Sort tokens by probability; Add tokens to the set until cumulative probability > p; Renormalize probabilities; Sample from this dynamic set

**Comparison:**
- Top-$$k$$ uses a fixed number of tokens, while Top-$$p$$ adapts based on the probability distribution.
- Top-$$p$$ can be more flexible, especially when the model is very confident or very uncertain.

Sampling strategies balance between diversity and quality of generated text. The choice of sampling method and its parameters can significantly affect the output, especially for longer sequences. Combining methods (e.g., Top-$$p$$ with a high-$$k$$ cutoff) can yield good results.`;

        const x_tok_t_plus_1_info = `$$x_{tok}^{(t+1)}$$ represents the next token in the sequence, sampled from the probability distribution output by the model.

**Key points:**
1. **Sampling process**: This token is chosen based on the probability distribution $$\\hat{P}( x_{tok}^{(t+1)} | x_{tok}^{(...t)} )$$ using a sampling strategy (e.g., greedy, top-k, top-p).

2. **Token ID**: It's an integer corresponding to a token in the model's vocabulary.

3. **Auto-regressive nature**: 
   - In training: The model learns to predict $$x_{tok}^{(t+1)}$$ given $$x_{tok}^{(...t)}$$. This next token is known from the training data.
   - In inference: The predicted token becomes part of the input for the next iteration.

4. **Stopping criteria**: Generation typically continues until a stopping condition is met (e.g., max length, special end token).`;


        const svgContent = {
            // 'root': [
            //     // 0. Data preparation and tokenization
            //     // 1. x_tokenized
            //     // 2a. Positional Encoding
            //     // 2b. Token Embedding (Tokenizer)
            //     // 3. Dropout
            //     // 4. LayerNorm
            //     // 5. Repeated Self-Attention Blocks
            //     // 6. Feed Forward MLP
            //     // 7. Residual Connections 
            //     // 8. Configurations (n_embd, block_size, activation fn, etc.)
            // ],
            // 'tokenization': {
            //     boxes: [
            //         {
            //             x: 50 + MIN_SEP_HORIZONTAL, y: 130, text: 'tok', width: SINGLE_VECTOR_WIDTH, height: SINGLE_VECTOR_HEIGHT,
            //             info: x_tok_info, link: x_tok_link, opacity: 0.4
            //         },
            //     ],
            //     references: [
            //         {
            //             title: "\"Let's build the GPT Tokenizer\"",
            //             authors: ["Andrej Karpathy"],
            //             link: "https://www.youtube.com/watch?v=zduSFxRajkE",
            //             info: "A detailed video tutorial on tokenization in GPT models",
            //             refType: "YouTube"
            //         }
            //     ]
            // },
            // 'position_encoding': {
            //     boxes: [
            //         {
            //             x: 50 + MIN_SEP_HORIZONTAL, y: 130, text: 'posemb', width: SINGLE_VECTOR_WIDTH, height: SINGLE_VECTOR_HEIGHT,
            //             info: x_tok_info, link: x_tok_link, opacity: 0.4
            //         },
            //     ],
            // },
            'attention': {
                textLabels: [
                    {
                        x: 50 + SINGLE_VECTOR_WIDTH + 40,
                        y: 130 + (SINGLE_VECTOR_HEIGHT / 2) - 10,
                        text: '"I tried to pay attention, but attention paid me" - Lil Wayne',
                        color: LATEX_LABEL_COLOR,
                        info: "Self attention stuff goes here",
                    }
                ]

            },
            'root': {
                boxes: [
                    {
                        x: 50, y: 130, text: '', width: SINGLE_VECTOR_WIDTH, height: SINGLE_VECTOR_HEIGHT * 2,
                        info: x_raw_info, link: x_raw_link, opacity: 0.4
                    },
                    {
                        x: 50 + MIN_SEP_HORIZONTAL, y: 130, text: '', width: SINGLE_VECTOR_WIDTH, height: SINGLE_VECTOR_HEIGHT,
                        info: x_tok_info, link: x_tok_link, opacity: 0.4
                    },
                    {
                        x: 50 + 2 * MIN_SEP_HORIZONTAL, y: 130, text: '', width: SINGLE_VECTOR_WIDTH * 4, height: SINGLE_VECTOR_HEIGHT,
                        info: x_embd_info, link: x_embd_link, opacity: 0.4
                    },
                    {
                        x: 50 + 3 * MIN_SEP_HORIZONTAL - (SINGLE_LATEX_CHAR_WIDTH / 2),
                        y: 130 + (SINGLE_VECTOR_HEIGHT / 2) - (SINGLE_LATEX_CHAR_HEIGHT / 2),
                        text: '', width: SINGLE_LATEX_CHAR_WIDTH, height: SINGLE_LATEX_CHAR_HEIGHT,
                        info: addition_info,
                        opacity: 0.2
                    },
                    {
                        x: 50 + 3 * MIN_SEP_HORIZONTAL + 150,
                        y: 130 + (SINGLE_VECTOR_HEIGHT / 2) - (50),
                        text: 'Succesive applications of self-attention blocks',
                        width: 350, height: 100,
                        info: attn_info,
                        link: attn_link,
                        opacity: 1
                    },
                    {
                        x: 50 + 4 * MIN_SEP_HORIZONTAL + 450,
                        y: 130 + (SINGLE_VECTOR_HEIGHT / 2) - (50),
                        text: 'Logits',
                        width: 100, height: 100,
                        opacity: 0.4,
                        info: logits_info
                    },
                    {
                        x: 50 + 5 * MIN_SEP_HORIZONTAL + 450,
                        y: 130 + (SINGLE_VECTOR_HEIGHT / 2) - (50),
                        text: 'Output probabilities for next token',
                        width: 280, height: 100,
                        opacity: 0.4,
                        info: est_prob_dist_info
                    },
                    {
                        x: 50 + 5 * MIN_SEP_HORIZONTAL + 830,
                        y: 130 + (SINGLE_VECTOR_HEIGHT / 2) - (SINGLE_LATEX_CHAR_HEIGHT / 2) - 10,
                        text: '', width: SINGLE_LATEX_CHAR_WIDTH + 10, height: SINGLE_LATEX_CHAR_HEIGHT + 10,
                        info: x_tok_t_plus_1_info,
                        opacity: 0.4
                    },
                ],
                latexLabels: [
                    {
                        x: 50 + (SINGLE_VECTOR_WIDTH / 2), y: 100,
                        text: 'x_{raw}',
                        color: LATEX_LABEL_COLOR, width: SINGLE_LATEX_CHAR_WIDTH, height: SINGLE_LATEX_CHAR_HEIGHT,
                        info: x_raw_info, link: x_raw_link
                    },
                    {
                        x: 50 + MIN_SEP_HORIZONTAL + (SINGLE_VECTOR_WIDTH / 2), y: 100,
                        text: 'x_{tok}',
                        color: LATEX_LABEL_COLOR, width: SINGLE_LATEX_CHAR_WIDTH, height: SINGLE_LATEX_CHAR_HEIGHT,
                        info: x_tok_info, link: x_tok_link
                    },
                    {
                        x: 50 + 2 * MIN_SEP_HORIZONTAL + (SINGLE_VECTOR_WIDTH * 2), y: 100,
                        text: 'x_{embd}',
                        color: LATEX_LABEL_COLOR, width: SINGLE_LATEX_CHAR_WIDTH, height: SINGLE_LATEX_CHAR_HEIGHT,
                        info: x_embd_info, link: x_embd_link
                    },
                    {
                        x: 50 + 3 * MIN_SEP_HORIZONTAL, y: 130 + (SINGLE_VECTOR_HEIGHT / 2) - 8,
                        text: '\\boldsymbol{+}',
                        width: SINGLE_LATEX_CHAR_WIDTH, height: SINGLE_LATEX_CHAR_HEIGHT, info: addition_info
                    },
                    {
                        x: 50 + 3 * MIN_SEP_HORIZONTAL + 985,
                        y: 250,
                        text: '\\hat{P}( x_{tok}^{(t+1)} | x_{tok}^{(...t)} )',
                        color: LATEX_LABEL_COLOR, width: 4 * SINGLE_LATEX_CHAR_WIDTH, height: SINGLE_LATEX_CHAR_HEIGHT,
                        info: est_prob_dist_info
                    },
                    {
                        x: 50 + 3 * MIN_SEP_HORIZONTAL + 1262,
                        y: 130 + (SINGLE_VECTOR_HEIGHT / 2) - 12,
                        text: 'x_{tok}^{(t+1)}',
                        color: LATEX_LABEL_COLOR, width: SINGLE_LATEX_CHAR_WIDTH, height: SINGLE_LATEX_CHAR_HEIGHT,
                        info: x_tok_t_plus_1_info
                    },
                    // { x: 450, y: 250, text: '\\hat{y}', color: '#ffffff', info: "Output prediction", link: 'box1' }
                ],
                textLabels: [
                    {
                        x: 50 + SINGLE_VECTOR_WIDTH + 40,
                        y: 130 + (SINGLE_VECTOR_HEIGHT / 2) - 10,
                        text: 'Tokenization',
                        color: LATEX_LABEL_COLOR,
                        info: tokenization_info,
                        link: tokenization_link
                    },
                    {
                        x: 50 + MIN_SEP_HORIZONTAL + SINGLE_VECTOR_WIDTH + 20,
                        y: 130 + (SINGLE_VECTOR_HEIGHT / 2) - 10,
                        text: 'Token Embedding',
                        color: LATEX_LABEL_COLOR,
                        info: token_embd_info,
                        link: token_embd_link
                    },
                    {
                        x: 50 + 3 * MIN_SEP_HORIZONTAL - SINGLE_LATEX_CHAR_WIDTH - 20,
                        y: 130 + (SINGLE_VECTOR_HEIGHT / 2) + 260,
                        text: 'Positional Encoding',
                        color: "#555", //LATEX_LABEL_COLOR,
                        info: pos_enc_info,
                        link: pos_enc_link
                    },
                    {
                        x: 50 + 3 * MIN_SEP_HORIZONTAL + 50,
                        y: 130 + (SINGLE_VECTOR_HEIGHT / 2) - 10,
                        text: '(Dropout)',
                        color: '#444',
                        info: dropout_info
                    },
                    {
                        x: 50 + 3 * MIN_SEP_HORIZONTAL + 512,
                        y: 130 + (SINGLE_VECTOR_HEIGHT / 2) - 10,
                        text: 'Linear Projection',
                        color: LATEX_LABEL_COLOR,
                        info: lin_prj_info
                    },
                    {
                        x: 50 + 3 * MIN_SEP_HORIZONTAL + 770,
                        y: 130 + (SINGLE_VECTOR_HEIGHT / 2) - 10,
                        text: 'Softmax',
                        color: LATEX_LABEL_COLOR,
                        info: softmax_info

                    },
                    {
                        x: 50 + 3 * MIN_SEP_HORIZONTAL + 1145,
                        y: 130 + (SINGLE_VECTOR_HEIGHT / 2) - 10,
                        text: 'Sampling',
                        color: LATEX_LABEL_COLOR,
                        info: sampling_info

                    },
                ],
                arrows: [
                    {
                        start: { x: 50 + SINGLE_VECTOR_WIDTH, y: 130 + (SINGLE_VECTOR_HEIGHT / 2) },
                        end: { x: 50 + MIN_SEP_HORIZONTAL, y: 130 + (SINGLE_VECTOR_HEIGHT / 2) },
                        color: ARROW_COLOR,
                        info: tokenization_info,
                        link: tokenization_link
                    },
                    {
                        start: { x: 50 + MIN_SEP_HORIZONTAL + SINGLE_VECTOR_WIDTH, y: 130 + (SINGLE_VECTOR_HEIGHT / 2) },
                        end: { x: 50 + 2 * MIN_SEP_HORIZONTAL, y: 130 + (SINGLE_VECTOR_HEIGHT / 2) },
                        color: ARROW_COLOR,
                        info: token_embd_info,
                        link: token_embd_link
                    },
                    {
                        start: { x: 50 + 2 * MIN_SEP_HORIZONTAL + 4 * SINGLE_VECTOR_WIDTH, y: 130 + (SINGLE_VECTOR_HEIGHT / 2) },
                        end: { x: 50 + 3 * MIN_SEP_HORIZONTAL - SINGLE_LATEX_CHAR_WIDTH / 2, y: 130 + (SINGLE_VECTOR_HEIGHT / 2) },
                        color: ARROW_COLOR, info: pos_encoding_arrows_info,
                    },
                    {
                        start: { x: 50 + 3 * MIN_SEP_HORIZONTAL, y: 130 + (SINGLE_VECTOR_HEIGHT / 2) + 170 },
                        end: { x: 50 + 3 * MIN_SEP_HORIZONTAL, y: 130 + (SINGLE_VECTOR_HEIGHT / 2) + SINGLE_LATEX_CHAR_HEIGHT / 2 },
                        color: "#333",
                        // color: ARROW_COLOR, 
                        info: pos_encoding_arrows_info,
                    },
                    {
                        start: { x: 50 + 3 * MIN_SEP_HORIZONTAL + 25, y: 130 + (SINGLE_VECTOR_HEIGHT / 2) },
                        end: { x: 50 + 3 * MIN_SEP_HORIZONTAL + 150, y: 130 + (SINGLE_VECTOR_HEIGHT / 2) },
                        color: ARROW_COLOR,
                        info: dropout_info
                    },
                    {
                        start: { x: 50 + 3 * MIN_SEP_HORIZONTAL + 500, y: 130 + (SINGLE_VECTOR_HEIGHT / 2) },
                        end: { x: 50 + 3 * MIN_SEP_HORIZONTAL + 650, y: 130 + (SINGLE_VECTOR_HEIGHT / 2) },
                        color: ARROW_COLOR,
                        info: lin_prj_info
                    },
                    {
                        start: { x: 50 + 3 * MIN_SEP_HORIZONTAL + 750, y: 130 + (SINGLE_VECTOR_HEIGHT / 2) },
                        end: { x: 50 + 3 * MIN_SEP_HORIZONTAL + 850, y: 130 + (SINGLE_VECTOR_HEIGHT / 2) },
                        color: ARROW_COLOR,
                        info: softmax_info
                    },
                    {
                        start: { x: 50 + 3 * MIN_SEP_HORIZONTAL + 1130, y: 130 + (SINGLE_VECTOR_HEIGHT / 2) },
                        end: { x: 50 + 3 * MIN_SEP_HORIZONTAL + 1230, y: 130 + (SINGLE_VECTOR_HEIGHT / 2) },
                        color: ARROW_COLOR,
                        info: sampling_info
                    },
                ],

                separators: [
                    {
                        x: 220,
                        // y1: 100,
                        // y2: 700,
                        type: 'dashed',
                        info: 'This seperates the tokenization process (left) from the transformer architecture (right)',
                        color: '#444'
                    },
                    {
                        x: 1790,
                        // y1: 100,
                        // y2: 700,
                        type: 'dashed',
                        info: 'This seperates the transformer architecture (left) from the token sampling process (right)',
                        color: '#444'
                    },
                ],

                paths: [
                    {
                        x: 50 + 3 * MIN_SEP_HORIZONTAL - SINGLE_LATEX_CHAR_WIDTH, y: 130 + (SINGLE_VECTOR_HEIGHT / 2) + 150,
                        width: 100, height: 100, pathD: positionalEncodingPath, color: POS_ENC_COLOR,
                        info: pos_enc_info,
                        link: pos_enc_link
                    }
                ],

            },
        };
        // END SVG Definition
        //-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=

        //-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
        // SVG Rendering and Event Handling
        //-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
        function drawContent(view) {
            contentGroup.selectAll('*').remove();

            if (svgContent[view]) {
                // Render boxes
                if (svgContent[view].boxes) {
                    svgContent[view].boxes.forEach(item => {
                        const rect = contentGroup.append('rect')
                            .attr('x', item.x)
                            .attr('y', item.y)
                            .attr('width', item.width)
                            .attr('height', item.height)
                            .attr('fill', RECT_FILL_COLOR)
                            .attr('stroke', RECT_STROKE_COLOR)
                            .attr('opacity', item.opacity !== undefined ? item.opacity : 1);

                        contentGroup.append('text')
                            .attr('x', item.x + item.width / 2)
                            .attr('y', item.y + item.height / 2)
                            .attr('text-anchor', 'middle')
                            .attr('dominant-baseline', 'central')
                            .attr('fill', TEXT_COLOR)
                            // .attr('opacity', item.opacity !== undefined ? item.opacity : 1)
                            .text(item.text);
                    });
                }

                // Render directed arrow connections
                if (svgContent[view].arrows) {
                    svgContent[view].arrows.forEach(arrow => {
                        const arrowElement = drawArrow(arrow.start, arrow.end, arrow.color);
                        arrowElement.attr('data-info', arrow.info)
                            .attr('data-link', arrow.link);
                    });
                }

                // Render separators
                if (svgContent[view].separators) {
                    svgContent[view].separators.forEach(separator => {
                        drawSeparator(separator);
                    });
                }

                // Render LaTeX labels
                if (svgContent[view].latexLabels) {
                    svgContent[view].latexLabels.forEach(label => {
                        contentGroup.append(() => createLatexLabel(
                            label.x,
                            label.y,
                            label.text,
                            label.color,
                            label.info,
                            label.link,
                            label.width,
                            label.height
                        ));
                    });
                }

                // Render text labels
                if (svgContent[view].textLabels) {
                    svgContent[view].textLabels.forEach(label => {
                        contentGroup.append(() => createTextLabel(
                            label.x,
                            label.y,
                            label.text,
                            label.color,
                            label.info,
                            label.link
                        ));
                    });
                }

                // Render custom paths
                if (svgContent[view].paths) {
                    svgContent[view].paths.forEach(path => {
                        contentGroup.append('path')
                            .attr('d', path.pathD)
                            .attr('transform', `translate(${path.x}, ${path.y}) scale(${path.width / 30}, ${path.height / 30})`)
                            .attr('fill', 'none')
                            .attr('stroke', path.color)
                            .attr('stroke-width', 2)
                            .attr('class', 'custom-path')
                            .attr('data-info', path.info)
                            .attr('data-link', path.link)
                    });
                }

                attachEventListenersForView(view);
            } else {
                logMessage(`No content found for view: ${view}`);
            }
        }


        function attachEventListenersForView(view) {
            // Add listeners to boxes
            if (svgContent[view].boxes) {
                svgContent[view].boxes.forEach(item => {
                    const rect = contentLayer.select(`rect[x="${item.x}"][y="${item.y}"]`);
                    const text = contentLayer.select(`text[x="${item.x + item.width / 2}"][y="${item.y + item.height / 2}"]`);

                    addDefaultEventListeners(rect.node(), item.link);
                    addDefaultEventListeners(text.node(), item.link);

                    if (item.link) {
                        const originalRectStroke = rect.attr('stroke');
                        const originalTextFill = text.attr('fill');

                        rect.on('mouseover', function () { rect.attr('stroke', LINK_COLOR); })
                            .on('mouseout', function () { rect.attr('stroke', originalRectStroke); });

                        text.on('mouseover', function () { rect.attr('stroke', LINK_COLOR); })
                            .on('mouseout', function () { rect.attr('stroke', originalTextFill); });
                    }
                });
            }

            // Add listeners to latex labels
            if (svgContent[view].latexLabels) {
                contentLayer.selectAll('.latex-label').each(function () {
                    const label = d3.select(this);
                    const link = label.attr('data-link');
                    addDefaultEventListeners(this, link);
                    if (link) {
                        const originalColor = label.select('div').style('color');
                        label.on('mouseover', function () { label.select('div').style('color', LINK_COLOR); })
                            .on('mouseout', function () { label.select('div').style('color', originalColor); });
                    }
                });
            }

            // Add listeners to text labels
            if (svgContent[view].textLabels) {
                contentLayer.selectAll('.text-label').each(function () {
                    const label = d3.select(this);
                    const link = label.attr('data-link');
                    addDefaultEventListeners(this, link);
                    if (link) {
                        const originalFill = label.attr('fill');
                        label.on('mouseover', function () {
                            label.attr('fill', LINK_COLOR);
                            showInfo({ currentTarget: this });
                        })
                            .on('mouseout', function () {
                                label.attr('fill', originalFill);
                                clearInfo();
                            })
                    }
                });
            }

            // Add listeners to arrows
            if (svgContent[view].arrows) {
                contentLayer.selectAll('.arrow').each(function () {
                    const arrow = d3.select(this);
                    const link = arrow.attr('data-link');
                    addDefaultEventListeners(arrow.node(), link);
                    if (link) {
                        const originalColor = arrow.attr('stroke');
                        arrow.on('mouseover', function () {
                            arrow.attr('stroke', LINK_COLOR).attr('fill', LINK_COLOR);
                        })
                            .on('mouseout', function () {
                                arrow.attr('stroke', originalColor).attr('fill', originalColor);
                            });
                    }
                });
            }

            // Add listeners to separators
            if (svgContent[view].separators) {
                contentLayer.selectAll('line').each(function () {
                    const separator = d3.select(this);
                    addDefaultEventListeners(separator.node());
                });
            }

            // Add listeners to custom paths
            if (svgContent[view].paths) {
                contentLayer.selectAll('.custom-path').each(function () {
                    const path = d3.select(this);
                    const link = path.attr('data-link');
                    addDefaultEventListeners(path.node(), link);
                    if (link) {
                        const originalStroke = path.attr('stroke');
                        path.on('mouseover', function () { path.attr('stroke', LINK_COLOR); })
                            .on('mouseout', function () { path.attr('stroke', originalStroke); });
                    }
                });
            }

            // Add listeners to navigation buttons
            navigationLayer.selectAll('.nav-button').each(function () {
                addDefaultEventListeners(this);
            });
        }


        function addDefaultEventListeners(element, link) {
            element.addEventListener('mouseover', showInfo);
            element.addEventListener('mouseout', clearInfo);
            element.addEventListener('click', function (event) {
                if (link) {
                    navigateTo(link);
                }
                // const infoElement = document.getElementById('info');
                // navigator.clipboard.writeText(infoElement.textContent).then(function () {
                //     logMessage("Text has been copied to clipboard");
                // }).catch(function (err) {
                //     logMessage("Failed to copy text: ", err);
                // });
                clearInfo(event);
            });

            // Keyboard events
            element.addEventListener('focus', showInfo);
            element.addEventListener('blur', clearInfo);

            // Update cursor style
            element.style.cursor = link ? 'pointer' : 'default';
        }


        function drawArrow(start, end, color) {
            const arrowSize = 10;
            const angle = Math.atan2(end.y - start.y, end.x - start.x);

            // Calculate the position of the arrowhead
            const arrowX = end.x - arrowSize * Math.cos(angle);
            const arrowY = end.y - arrowSize * Math.sin(angle);

            // Create a group for the arrow
            const arrowGroup = contentGroup.append('g')
                .attr('class', 'arrow')
                .attr('stroke', color)
                .attr('fill', color);

            // Draw the line
            arrowGroup.append('line')
                .attr('x1', start.x)
                .attr('y1', start.y)
                .attr('x2', end.x)
                .attr('y2', end.y)
                .attr('stroke-width', 2);

            // Draw the arrowhead
            arrowGroup.append('path')
                .attr('d', `M${end.x},${end.y} L${arrowX - arrowSize * Math.cos(angle - Math.PI / 6)},${arrowY - arrowSize * Math.sin(angle - Math.PI / 6)} L${arrowX - arrowSize * Math.cos(angle + Math.PI / 6)},${arrowY - arrowSize * Math.sin(angle + Math.PI / 6)} Z`);

            return arrowGroup;
        }


        function drawSeparator(separator) {
            const line = contentGroup.append('line')
                .attr('stroke', separator.color || '#888')
                .attr('stroke-width', 1);

            if (separator.x !== undefined) {
                // Vertical line
                line.attr('x1', separator.x)
                    .attr('x2', separator.x)
                    .attr('y1', separator.y1 || -SINGLE_VECTOR_HEIGHT)
                    .attr('y2', separator.y2 || svgHeight + SINGLE_VECTOR_HEIGHT);
            } else if (separator.y !== undefined) {
                // Horizontal line
                line.attr('y1', separator.y)
                    .attr('y2', separator.y)
                    .attr('x1', separator.x1 || -100)
                    .attr('x2', separator.x2 || 2 * svgWidth);
            }

            if (separator.type === 'dashed') {
                line.attr('stroke-dasharray', '5,5');
            } else if (separator.type === 'dotted') {
                line.attr('stroke-dasharray', '2,2');
            }

            line.attr('data-info', separator.info || "Separator");
        }

        // END SVG Rendering and Event Handling
        //-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=

        //-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
        // SVG Zoom
        //-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
        function setupZoom() {
            zoom = d3.zoom()
                .scaleExtent([0.4, 2])
                .on('zoom', zoomed);

            const svg = d3.select("#svg")
                .call(zoom)
                .on("dblclick.zoom", null);

            contentGroup = contentLayer
                .append("g")
                .attr("class", "content-group");

            // Store the initial transform
            zoom.transform(svg, d3.zoomIdentity);
        }

        function zoomed(event) {
            contentGroup.attr('transform', event.transform);
        }

        function resetZoom() {
            svg.transition().duration(750).call(
                zoom.transform,
                d3.zoomIdentity,
                d3.zoomTransform(svg.node()).invert([svgWidth / 2, svgHeight / 2])
            );
        }
        // END SVG Zoom
        //-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=

        //-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
        // SVG Navigation
        //-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
        function drawNavigation() {
            navigationLayer.selectAll('.nav-button').remove();
            drawArrowButton('back-button', 5, 5, arrowPaths.back, navigateBack, undoNavigationHistory.length > 0);
            drawArrowButton('forward-button', 55, 5, arrowPaths.forward, navigateForward, redoNavigationHistory.length > 0);
            drawArrowButton('reset-button', svgWidth - 45, 5, arrowPaths.reset, resetZoom, true);
        }


        function drawArrowButton(id, x, y, pathD, clickHandler, isEnabled) {
            const group = navigationLayer.append('g')
                .attr('class', 'nav-button')
                .attr('id', id)
                .attr('transform', `translate(${x}, ${y})`)
                .style('cursor', isEnabled ? 'pointer' : 'default');

            // Button background
            group.append('rect')
                .attr('width', 40)
                .attr('height', 40)
                .attr('rx', 5)
                .attr('ry', 5)
                .attr('fill', isEnabled ? BUTTON_FILL_COLOR : DISABLED_BUTTON_FILL_COLOR)
                .attr('stroke', isEnabled ? BUTTON_STROKE_COLOR : DISABLED_BUTTON_STROKE_COLOR);

            // Arrow
            group.append('path')
                .attr('d', pathD)
                .attr('fill', 'none')
                .attr('stroke', isEnabled ? BUTTON_ARROW_COLOR : DISABLED_BUTTON_ARROW_COLOR)
                .attr('stroke-width', 2)
                .attr('stroke-linecap', 'round')
                .attr('stroke-linejoin', 'round')
                .attr('transform', 'translate(5, 5)');

            if (isEnabled) {
                group.on('click', clickHandler);

                // Hover effect
                group.on('mouseover', function () {
                    d3.select(this).select('rect').attr('fill', BUTTON_HOVER_FILL_COLOR);
                    d3.select(this).select('path').attr('stroke', BUTTON_HOVER_ARROW_COLOR);
                });

                group.on('mouseout', function () {
                    d3.select(this).select('rect').attr('fill', BUTTON_FILL_COLOR);
                    d3.select(this).select('path').attr('stroke', BUTTON_ARROW_COLOR);
                });
            }
        }


        function navigateTo(view) {
            if (currentView == view) {
                logMessage("repeat navigateTo. exiting");
                return;
            }
            undoNavigationHistory.push(currentView);
            redoNavigationHistory = []; // Clear redo history
            currentView = view;
            updateView();
            resetZoom();
        }

        function navigateBack() {
            if (undoNavigationHistory.length > 0) {
                redoNavigationHistory.push(currentView);
                currentView = undoNavigationHistory.pop();
                updateView();
                resetZoom();
            }
        }

        function navigateForward() {
            if (redoNavigationHistory.length > 0) {
                undoNavigationHistory.push(currentView);
                currentView = redoNavigationHistory.pop();
                updateView();
                resetZoom();
            }
        }

        function updateView() {
            drawNavigation();
            drawContent(currentView);
        }
        // END SVG Navigation
        //-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=

        //-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
        // LaTeX and plaintext labels
        //-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
        function createLatexLabel(x, y, text, color, labelInfo, link, width = 250, height = 40) {
            const foreignObject = document.createElementNS('http://www.w3.org/2000/svg', 'foreignObject');
            foreignObject.setAttribute('x', x - (width / 2));
            foreignObject.setAttribute('y', y - 20);
            foreignObject.setAttribute('width', width);
            foreignObject.setAttribute('height', height);
            foreignObject.classList.add('latex-label');
            foreignObject.dataset.latex = text;
            foreignObject.dataset.info = labelInfo;

            if (link) {
                foreignObject.dataset.link = link;
                // foreignObject.style.cursor = 'pointer';
            }

            const div = document.createElement('div');
            div.style.fontSize = LATEX_FONT_SIZE + 'px';
            div.style.color = color;
            div.style.display = 'flex';
            div.style.justifyContent = 'center';
            div.style.alignItems = 'center';
            div.style.height = '100%';

            katex.render(text, div, {
                throwOnError: true
            });

            foreignObject.appendChild(div);

            return foreignObject;
        }


        function createTextLabel(x, y, text, color, labelInfo, link) {
            const textElement = document.createElementNS('http://www.w3.org/2000/svg', 'text');
            textElement.setAttribute('x', x);
            textElement.setAttribute('y', y);
            textElement.setAttribute('fill', color);
            textElement.textContent = text;
            textElement.classList.add('text-label');
            textElement.dataset.info = labelInfo;
            // textElement.setAttribute('data-info', labelInfo);
            if (link) {
                textElement.dataset.link = link;
                // textElement.setAttribute('data-link', link);
            }
            return textElement;
        }
        // END LaTeX and plaintext labels
        //-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=

        //-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
        // Display detailed info top-right
        //-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
        function showInfo(event) {
            const target = event.currentTarget;
            let content = '';

            if (target.tagName === 'A' && target.parentElement.parentElement.id === 'references-list') {
                const key = target.textContent.split(" (")[0]
                const ref = references.find(r => r.title === key);
                if (ref) {
                    content = formatReferenceInfo(ref);
                }
            } else if (target.classList.contains('custom-path')) {
                content = target.getAttribute('data-info') || "Custom path";
            } else if (target.classList.contains('arrow')) {
                content = target.getAttribute('data-info') || "Arrow connection";
            } else if (target.classList.contains('nav-button')) {
                if (target.id === 'back-button') {
                    content = "Navigate back to the previous view.";
                } else if (target.id === 'forward-button') {
                    content = "Navigate forward to the next view.";
                } else if (target.id === 'reset-button') {
                    content = "Reset zoom and pan to the initial position.";
                }
            } else if (target.classList.contains('latex-label')) {
                // content = target.dataset.info;
                content = target.getAttribute('data-info');
            } else if (target.classList.contains('text-label')) {
                content = target.getAttribute('data-info');
            } else if (target.tagName === 'rect' || target.tagName === 'text') {
                const item = svgContent[currentView].boxes.find(i =>
                    i.x == target.getAttribute('x') ||
                    (target.tagName === 'text' && i.x + i.width / 2 == target.getAttribute('x'))
                );
                if (item && item.info) {
                    content = item.info;
                }
            } else if (target.tagName === 'line') {
                content = target.getAttribute('data-info') || "Separator";
            }
            renderInfoContent(document.getElementById('info'), content);
        }


        function renderInfoContent(element, content) {
            element.innerHTML = '';
            let currentIndex = 0;

            while (currentIndex < content.length) {
                if (content.startsWith('$$', currentIndex)) {
                    // LaTeX content
                    const endIndex = content.indexOf('$$', currentIndex + 2);
                    if (endIndex === -1) {
                        console.error('Unclosed LaTeX at position', currentIndex);
                        break;
                    }
                    const latexContent = content.slice(currentIndex + 2, endIndex);
                    const span = document.createElement('span');
                    katex.render(latexContent, span, {
                        throwOnError: false,
                        displayMode: false
                    });
                    element.appendChild(span);
                    currentIndex = endIndex + 2;
                } else if (content.startsWith('**', currentIndex)) {
                    // Bold content
                    const endIndex = content.indexOf('**', currentIndex + 2);
                    if (endIndex === -1) {
                        console.error('Unclosed bold at position', currentIndex);
                        break;
                    }
                    const boldContent = content.slice(currentIndex + 2, endIndex);
                    const strong = document.createElement('strong');
                    strong.textContent = boldContent;
                    element.appendChild(strong);
                    currentIndex = endIndex + 2;
                } else {
                    // Plain text
                    const nextSpecialChar = Math.min(
                        content.indexOf('$$', currentIndex) === -1 ? Infinity : content.indexOf('$$', currentIndex),
                        content.indexOf('**', currentIndex) === -1 ? Infinity : content.indexOf('**', currentIndex)
                    );
                    const textContent = content.slice(currentIndex, nextSpecialChar === Infinity ? undefined : nextSpecialChar);
                    const lines = textContent.split('\n');
                    lines.forEach((line, index) => {
                        element.appendChild(document.createTextNode(line));
                        if (index < lines.length - 1) {
                            element.appendChild(document.createElement('br'));
                        }
                    });
                    currentIndex = nextSpecialChar === Infinity ? content.length : nextSpecialChar;
                }
            }
        }

        function clearInfo(event) {
            const defaultStr = "";
            document.getElementById('info').textContent = defaultStr;
        }
        // END Display detailed info top-right
        //-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=

        //-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
        // Display references bot right
        //-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
        function formatReferenceInfo(ref) {
            let content = "";
            content += `**Reference Name:** ${ref.title}\n\n`;
            content += `**Description:** ${ref.info}\n\n`
            content += '**Authors:**\n';
            ref.authors.forEach(author => {
                content += `• ${author}\n`;
            });
            content += `\n**Link type:** ${ref.refType}\n\n`;
            return content;
        }

        function renderReferences() {
            const referencesList = document.getElementById('references-list');
            referencesList.innerHTML = '';

            if (references) {
                references.forEach(ref => {
                    const li = document.createElement('li');
                    const a = document.createElement('a');
                    a.href = ref.link;
                    a.target = "_blank";
                    a.textContent = ref.title + ` (${ref.refType})`;
                    a.addEventListener('mouseover', showInfo);
                    a.addEventListener('mouseout', clearInfo);
                    li.appendChild(a);
                    referencesList.appendChild(li);
                });
            }
        }

        // For debugging or general message to user
        function logMessage(message) {
            const logContainer = document.getElementById('log-container');
            logContainer.innerHTML += `<p>${message}</p>`;
            logContainer.scrollTop = logContainer.scrollHeight;
        }

        //-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
        // Main
        //-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
        window.onload = function () {
            setupZoom();
            updateView();
            renderReferences();
        }
    </script>
</body>