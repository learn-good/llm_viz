<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Decoder-only transformer</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #1e1e1e;
            color: #e0e0e0;
        }

        #reporting-container {
            font-size: 16px;
            line-height: 1.5;
            font-family: 'Courier New', Courier, monospace;
            padding: 20px;
            width: 400px;
            /* background-color: #2a2a2a; */
        }

        #main-container {
            /* display: flex; */
            width: 100%;
            height: 800px;
        }

        #svg-container {
            /* flex: 0 0 1200px; */
            /* flex: 0 0 auto;
            z-index: 10; */
            float: left;
        }

        #svg {
            cursor: move;
        }


        #right-container {
            /* flex: 1; */
            float: right;
            display: flex;
            flex-direction: column;
            height: 840px;
        }

        .container {
            display: flex;
            justify-content: center;
        }

        #info-container {
            flex: 4;
            display: flex;
            flex-direction: column;
            padding: 10px;
        }

        #info {
            /* margin-bottom: 20px; */
            /* height: 550px; */
            flex: 1;
            padding: 10px;
            border: 1px solid #444;
            font-size: 16px;
            line-height: 1.5;
            border-radius: 15px;
            overflow: hidden;
            background-color: #2a2a2a;
        }

        #info .katex {
            font-size: 1.2em;
        }

        .nav-button {
            cursor: pointer;
        }

        .clickable {
            cursor: pointer;
        }

        /* Dark mode specific styles */
        svg {
            background-color: #2a2a2a;
            border-radius: 15px;
            padding: 10px;
            margin: 10px 0 10px 10px;
            border: 1px solid #444;
        }

        .latex-label:hover {
            cursor: default;
        }

        .latex-label[data-link]:hover {
            cursor: pointer;
        }

        /* References */
        #references-container {
            /* height: 250px; */
            flex: 1;
            display: flex;
            flex-direction: row;
            padding: 0px 10px 10px 10px;
        }

        #references {
            /* height: 220px; */
            flex: 1;
            padding: 10px;
            background-color: #2a2a2a;
            border: 1px solid #444;
            border-radius: 15px;
        }

        #references-container h3 {
            margin-top: 0;
            color: #b4b4b4;
        }

        #references-list {
            height: 150px;
            list-style-type: none;
            padding: 5px;
            overflow-y: auto;
            margin: 0;
        }

        #references-list li {
            margin-bottom: 10px;
        }

        #references-list a {
            color: #b4b4b4;
            text-decoration: none;
        }

        #references-list a:hover {
            color: #00ffff;
            text-decoration: underline;
        }


        /* text {
            fill: #e0e0e0;
        } */
    </style>
    <!-- for LaTeX -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js"></script>
    <!-- for plotting data -->
    <script src="https://d3js.org/d3.v7.min.js"></script>
</head>

<body>
    <!--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
        HTML
     -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-->
    <div id="main-container">
        <div id="svg-container">
            <svg id="svg" height="800">
                <g id="content-layer"></g>
                <g id="navigation-layer"></g>
            </svg>
        </div>
        <div id="right-container">
            <div id="info-container">
                <div id="info"></div>

            </div>
            <div id="references-container">
                <div id="references">
                    <h3>References</h3>
                    <ul id="references-list"></ul>
                </div>
            </div>
        </div>
    </div>
    <div id="reporting-container">
        <div id="log-container">
        </div>
    </div>

    <!--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
        Javascript
     -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-->
    <script>
        // Globals
        let zoom;
        let contentGroup;


        // SVG Navigation
        let currentView = 'root';
        let undoNavigationHistory = [];
        let redoNavigationHistory = [];

        // SVG paths for navigation arrows
        const arrowPaths = {
            up: "M5,20 L15,5 L25,20 Z",
            back: "M20,5 L5,15 L20,25 Z",
            forward: "M10,5 L25,15 L10,25 Z",
            reset: "M15,5 A10,10 0 1,0 15,25 A10,10 0 1,0 15,5 "
        };
        const repeatPath = "M15,5 A10,10 0 1,0 25,15 M25,10 L21,16 M25,10 L29,16";
        const positionalEncodingPath = "M15,5 A10,10 0 1,0 15,25 A10,10 0 1,0 15,5 M5,15 Q10,8 15,15 Q20,22 25,15";

        // Color constants
        const BACKGROUND_COLOR = '#2a2a2a';
        const RECT_FILL_COLOR = '#3a3a3a';
        const RECT_STROKE_COLOR = '#555';
        const TEXT_COLOR = '#e0e0e0';
        const BUTTON_FILL_COLOR = '#3a3a3a';
        const BUTTON_STROKE_COLOR = '#555';
        const BUTTON_ARROW_COLOR = '#e0e0e0';
        const BUTTON_HOVER_FILL_COLOR = '#4a4a4a';
        const BUTTON_HOVER_ARROW_COLOR = '#ffffff';
        const DISABLED_BUTTON_FILL_COLOR = '#2a2a2a';
        const DISABLED_BUTTON_STROKE_COLOR = '#3a3a3a';
        const DISABLED_BUTTON_ARROW_COLOR = '#4a4a4a';
        const LATEX_LINK_HOVER_COLOR = "#00ffff";
        const LINK_COLOR = '#00ffff';
        const LATEX_LABEL_COLOR = "#ffffff";
        const LATEX_LABEL_SECONDARY_COLOR = "#777";
        const LATEX_SHAPE_LABEL_COLOR = "#555";
        const ARROW_COLOR = "#444";
        const POS_ENC_COLOR = "#333";


        const LATEX_FONT_SIZE = 18;

        const references = [
            {
                title: "Attention Is All You Need",
                authors: [
                    "Ashish Vaswani",
                    "Noam Shazeer",
                    "Niki Parmari",
                    "Jakob Uszkoreit",
                    "Llion Jones",
                    "Aidan N. Gomez",
                    "Lukasz Kaiser",
                    "Illia Polosukhin",
                ],
                link: "https://arxiv.org/abs/1706.03762",
                info: "The original paper introducing the Transformer architecture.",
                refType: "arXiv"
            },
            {
                title: "nanoGPT",
                authors: ["Andrej Karpathy", "Github contributors"],
                link: "https://github.com/karpathy/nanoGPT/blob/master/model.py",
                info: "A minimal implementation of GPT in PyTorch.",
                refType: "Github"
            },
            {
                title: "llm.c",
                authors: ["Andrej Karpathy", "Github contributors"],
                link: "https://github.com/karpathy/llm.c",
                info: "\"LLMs in simple, pure C/CUDA with no need for 245MB of PyTorch or 107MB of cPython. Current focus is on pretraining, in particular reproducing the GPT-2 and GPT-3 miniseries, along with a parallel PyTorch reference implementation in train_gpt2.py.\"",
                refType: "Github"
            },
            {
                title: "\"Let's build GPT: from scratch, in code, spelled out\"",
                authors: ["Andrej Karpathy"],
                link: "https://www.youtube.com/watch?v=kCc8FmEb1nY",
                info: "A detailed video tutorial on building a GPT model from scratch.",
                refType: "YouTube"
            },
            {
                title: "\"Let's reproduce GPT-2\"",
                authors: ["Andrej Karpathy"],
                link: "https://www.youtube.com/watch?v=l8pRSuU81PU",
                info: "A video tutorial on reproducing the GPT-2 (124M) model.",
                refType: "YouTube"
            },
            {
                title: "\"Let's build the GPT Tokenizer\"",
                authors: ["Andrej Karpathy"],
                link: "https://www.youtube.com/watch?v=zduSFxRajkE",
                info: "A detailed video tutorial on tokenization in GPT models",
                refType: "YouTube"
            },
            {
                title: "Tiktoken",
                authors: ["OpenAI", "Shantanu Jain", "Github Contributors"],
                link: "https://github.com/openai/tiktoken",
                info: "A fast BPE tokeniser for use with OpenAI's models.",
                refType: "Github"
            },
            {
                title: "Tiktokenizer App",
                authors: ["David Duong"],
                link: "https://tiktokenizer.vercel.app/",
                info: "An interactive application to tokenize text using different tokenizers",
                refType: "web app"
            },
            {
                title: "Using the Output Embedding to Improve Language Models",
                authors: ["Ofir Press", "Lior Wolf"],
                link: "https://arxiv.org/abs/1608.05859",
                info: "A paper discussing techniques to improve language models by tying the token input embedding and the self-attention output embedding",
                refType: "arXiv"
            },
            {
                title: "The Llama 3 Herd of Models",
                authors: ["The Llama team at Meta"],
                link: "https://arxiv.org/abs/2407.21783",
                info: "A paper describing the Llama 3 family of large language models.",
                refType: "arXiv"
            },
            {
                title: "llama-models",
                authors: ["Meta", "Github contributors"],
                link: "https://github.com/meta-llama/llama-models",
                info: "Contains reference implementations and information about the Llama family of models.",
                refType: "arXiv"
            },
            {
                title: "RoFormer: Enhanced Transformer with Rotary Position Embedding",
                authors: [
                    "Jianlin Su",
                    "Yu Lu",
                    "Shengfeng Pan",
                    "Ahmed Murtadha",
                    "Bo Wen",
                    "Yunfeng Liu"
                ],
                link: "https://arxiv.org/abs/2104.09864",
                info: "A paper introducing Rotary Position Embedding, an alternative to traditional positional encodings in transformer models.",
                refType: "arXiv"
            },
            {
                title: "Rotary Positional Embeddings: Combining Absolute and Relative",
                authors: [
                    "Bai Li"
                ],
                link: "https://www.youtube.com/watch?v=o29P0Kpobz0",
                info: "A video explanation of RoPE - Rotary Positional Embeddings",
                refType: "YouTube"
            },
            {
                title: "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
                authors: [
                    "Nitish Srivastava",
                    "Geoffrey Hinton",
                    "Alex Krizhevsky",
                    "Ilya Sutskever",
                    "Ruslan Salakhutdinov"
                ],
                link: "https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf",
                info: "This seminal paper introduces the dropout technique, a simple yet effective method to reduce overfitting in neural networks by randomly 'dropping out' a proportion of neurons during training.",
                refType: "PDF"
            },
            {
                title: "Online normalizer calculation for softmax",
                authors: [
                    "Maxim Milakov",
                    "Natalia Gimelshein",
                ],
                link: "https://arxiv.org/abs/1805.02867",
                info: "Proposes an online, memory-access efficient algorithm for calculating the numerically stable softmax.",
                refType: "arXiv"
            },
            {
                title: "Deep Residual Learning for Image Recognition",
                authors: [
                    "Kaiming He",
                    "Xiangyu Zhang",
                    "Shaoqing Ren",
                    "Jian Sun"
                ],
                link: "https://arxiv.org/abs/1512.03385",
                info: "Introduces residual connections for use in deep neural networks, allowing for easier optimization of very deep networks.",
                refType: "arXiv"
            },
            {
                title: "Layer Normalization",
                authors: [
                    "Jimmy Lei Ba",
                    "Jamie Ryan Kiros",
                    "Geoffrey E. Hinton"
                ],
                link: "https://arxiv.org/abs/1607.06450",
                info: "This paper introduces Layer Normalization, a technique to normalize the inputs across the features, which helps in reducing the covariate shift problem in deep neural networks.",
                refType: "arXiv"
            },
            {
                title: "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
                authors: [
                    "Tri Dao",
                    "Daniel Y. Fu",
                    "Stefano Ermon",
                    "Atri Rudra",
                    "Christopher RÃ©"
                ],
                link: "https://arxiv.org/abs/2205.14135",
                info: "This paper presents FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce memory access and increase efficiency in transformer models.",
                refType: "arXiv"
            },
            {
                title: "Efficiently Scaling Transformer Inference",
                authors: [
                    "Reiner Pope",
                    "Sholto Douglas",
                    "Aakanksha Chowdhery",
                    "Jacob Devlin",
                    "James Bradbury",
                    "Anselm Levskaya",
                    "Jonathan Heek",
                    "Kefan Xiao",
                    "Shivani Agrawal",
                    "Jeff Dean"
                ],
                link: "https://arxiv.org/abs/2211.05102",
                info: "The paper that introduces the KV Cache, a technique to optimize transformer inference by caching key and value tensors from previous tokens, significantly reducing computation time for autoregressive generation.",
                refType: "arXiv"
            }
        ]

        //-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
        // SVG Definition
        //-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
        let currentSvgWidth = 1200;
        const svgHeight = 800;
        const svg = d3.select("#svg")
            .attr("width", currentSvgWidth)
            .attr("height", svgHeight)
            .style("background-color", BACKGROUND_COLOR);

        const contentLayer = d3.select("#content-layer");
        const navigationLayer = d3.select("#navigation-layer");
        // const hiddenContent = d3.select("#hidden-content");

        const SINGLE_LATEX_CHAR_HEIGHT = 50;
        const SINGLE_LATEX_CHAR_WIDTH = 50;
        const SINGLE_VECTOR_WIDTH = 24;
        const SINGLE_VECTOR_HEIGHT = 400;
        const MIN_SEP_HORIZONTAL = 200;

        // Rule for whether something can have a link:
        //    try to explain in allotted space with text
        //    if after that visual help still useful, make a link
        const x_raw_info = `$$x_{raw}$$ represents the raw input data for the transformer model.

**During training**: The input is usually a large corpus of text data, such as books, articles, or web pages. This can be several terabytes of data. It gets tokenized and split into manageable sequences (e.g., 1024 tokens) for batch processing.

**During inference**: The input is typically a prompt or partial sequence provided by the user. This can range from a few words to several paragraphs, depending on the task. This input is then tokenized and the model then generates a continuation based on this input.

The input can include various languages, special characters, and formatting, which the subsequent tokenization step will process.

For non-text data types, appropriate preprocessing steps would be applied before feeding the input into the transformer architecture.`
        const x_raw_link = "" // "x_raw"
        const x_tok_info = `The tokenized input $$x_{tok}$$ serves as the starting point for the transformer's processing, capturing the structure and content of the input text in a format that is usable by the neural network.

**Vocabulary size**: Tokens are drawn from a fixed vocabulary, e.g. $$\\text{vocabsize} = 64128$$.

**Special tokens**: These include tokens like \`[START]\`, \`[END]\`, \`[PAD]\`, \`[UNK]\` for start of sequence, end of sequence, padding, and unknown words respectively.

**Subword tokenization**: Most modern tokenizers (e.g., BPE, WordPiece, SentencePiece) use subword units.

**Sequence length**: Tokenized sequences are typically padded or truncated to a fixed length (e.g., 512 or 4096 tokens). We will refer to this number as $$\\text{seqlen}$$.

**Token IDs**: Each token is represented by a unique integer ID, which is used as input to the embedding layer.`
        const x_tok_link = "" //"x_tok"
        const x_embd_info = `$$x_{embd}$$ represents the embedded data that serves a the enriched input data for the transformer model.

**Shape**: $$x_{embd}$$ has shape $$(\\text{seqlen} \\times d_{embd})$$, where $$\\text{seqlen}$$ is the sequence length and $$d_{embd}$$ is the embedding dimension.

**Learned token embeddings**: The embedding matrix learns richer representations of tokens than just the token ID present in $$x_{tok}$$, capturing semantic meaning about the input tokens.

**Input to attention layers**: $$x_{embd} + PE$$ serves as the initial input to the self-attention mechanism in the transformer, where $$PE$$ is the positional encoding used to capture information about a token's position in the sequence.

**Why do we need to add positional encoding?**: The self-attention mechanism in transformers is permutation-invariant, meaning it treats the input as a set rather than a sequence. There are many approaches to address this limitation, such as absolute (pictured), relative, and rotary position embeddings.`;
        const x_embd_link = "" // "x_embd"
        const tokenization_info = `Tokenization bridges the gap between human-readable text and the numerical input required by transformer models. The tokenizer converts raw text into a sequence of integers, or token IDs, which the model can then process. The tokenizer is constructed separately from the rest of the transformer, potentially using different data.

**BPE** (Byte Pair Encoding) is a popular tokenization method that starts with individual characters and iteratively merges the most frequent pairs to form new tokens, while other approaches like WordPiece and SentencePiece offer alternative tokenization strategies.

**Some of the potential issues with tokenization**:
1. **Out-of-vocabulary words**: Rare or unseen words may be split into suboptimal subwords.

2. **Contextual meaning loss**: Splitting words into subwords and inadvertant groupings can sometimes cause loss of important context.

3. **Inconsistent tokenization**: The same word might be tokenized differently depending on the characters around it.`
        const tokenization_link = "" // "tokenization"
        const token_embd_info = `Token embedding converts tokenized input into dense vector representations, allowing for the model to learn a richer representation for each token.

**Lookup table**: Each token is mapped to a unique vector in a high-dimensional space, (e.g., $$d_{embd}=512$$, where $$d_{embd}$$ refers to the embedding dimension). When $$x_{tok}$$ is one-hot encoded, it is expanded such that the index pertaining the the token ID is set to $$1,$$ and all others are set to $$0,$$ which leads to that index \"looking up\" its higher dimensional representation via matrix multiplication.

**Learned representations**: These embeddings are learned during the training process, capturing semantic and syntactic information about tokens.

**Weight tying**: The embedding matrix is often shared with the final linear projection layer in the model's output, reducing the number of parameters and improving generalization. The intuition is that the process of generating a word (output) should be closely related to the process of understanding a word (input).

The embedding process can be represented as: 
$$x_{embd} = x_{tok} \\cdot W_{embd} $$

Where $$x_{tok}$$ is one-hot encoded from shape $$( \\text{seqlen} \\times 1 )$$ to $$( \\text{seqlen} \\times \\text{vocabsize} )$$, and $$W_{embd}$$ is the embedding matrix of shape: $$( \\text{vocabsize} \\times d_{embd} )$$. The result $$x_{embd}$$ is shape $$( \\text{seqlen} \\times d_{embd} )$$.`;

        const token_embd_link = "" //"token_embedding"
        const pos_enc_info = `The self-attention mechanism in transformers is permutation-invariant, meaning it doesn't inherently consider the order of tokens. $$PE$$ adds information about the position of each token in the sequence.

        The original sinusoidal $$PE$$ uses sine and cosine functions of different frequencies to create unique encodings for each position. This is one version of an absolute position encoding, where the encoding is based solely on the absolute position of the input.

**Alternative approaches:**
1. **Learned PE**: Trainable embeddings for each position.

2. **Relative PE**: Encodes relative distances between tokens instead of absolute positions.

3. **Rotary PE (RoPE)**: Applies rotation to query vector $$Q$$ and key matrix $$K$$ that are generated from the token embeddings during the self-attention process.

The choice of PE can significantly impact model performance and generalization ability, especially for tasks sensitive to token order or involving extrapolation to longer sequences.`;
        const pos_enc_link = "" // "position_encoding"
        const attn_info = `The successive applications of self-attention blocks form the core of the transformer architecture. Each block processes the input sequence, allowing the model to capture complex relationships between tokens. **Click for a more detailed breakdown**.

**Stacking multiple blocks:**
- The output of one block becomes the input to the next.
- Each successive layer can capture increasingly complex and abstract relationships in the token sequences.

**Key advantages:**
- Long-range dependencies: Can capture relationships between distant tokens.
- Contextual understanding: Each token's representation is influenced by the entire sequence.

The number of attention blocks significantly impacts the model's capacity and computational requirements. Deeper models (more blocks) can potentially capture more complex patterns but require more computational resources.`;
        const attn_link = "successive-self-attention"

        const logits_info = `Logits are the raw, unnormalized predictions that are output by the transformer model before the final softmax layer, representing the model's raw predictions for each token in the vocabulary.

**Shape**: The logits vector has length of $$\\text{vocabsize}$$.

**Interpretation**: Each value in the logits vector represents the model's unnormalized confidence for a particular token or class.

**Temperature**: Before converting to probabilities, logits are often divided by a temperature parameter to control the randomness of the output distribution.

The logits represent the model's raw decision space before it's constrained to a probability distribution, offering a view into the model's pre-normalized predictions.`;

        const dropout_info = `Dropout is a regularization technique used to prevent overfitting in neural networks, including transformers. It can be used at different locations in the network, or excluded altogether.

**Mechanism**: During training, randomly "drops out" (sets to zero) a proportion of neurons.

**Purpose**: Reduces co-adaptation between neurons, forcing the network to learn more robust features.

**Original paper**: The "Attention Is All You Need" paper used dropout in several places, including after the attention and feedforward layers, and in the embedding layers.

**Important note**: While dropout was used in the original transformer and is common in many models, it's not always necessary. For example, the Llama 3 family of models doesn't use dropout at all, demonstrating that high-performing models can be built without this technique.`;
        const lin_prj_info = `The linear projection layer serves as the final transformation to recover the vocab size dimension, $$\\text{vocabsize}$$, before the softmax operation to get next token probabilities.

**Purpose**: It maps the output of the self-attention blocks to logits of the vocabulary size.

**Weight tying**: In many implementations, $$W_{proj}$$ is often tied (shared) with the input embedding matrix, $$W_{embd}$$, which can improve performance and reduce the number of parameters.

The linear projection prepares the model's raw predictions (logits) for the final probability distribution over the vocabulary, which is then computed by the softmax function.`;

        const softmax_info = `The softmax function is applied to the logits to produce a probability distribution over the vocabulary for the next token.

**Formula and Temperature**: For input vector $$z$$, the softmax function is defined as:
   $$\\text{softmax}(z_i, T) = \\dfrac{e^{z_i/T}}{\\sum_{j=1}^K e^{z_j/T}}$$

   where $$K$$ is the number of classes ($$\\text{vocabsize}$$ in this case). A temperature parameter $$T$$ can be applied to control the "sharpness" of the distribution.
    - Lower T (<1) makes distribution more peaked (less random)
    - Higher T (>1) makes distribution more uniform (more random)

**Properties**: Output values are between $$0$$ and $$1$$; sum of all outputs equals $$1$$; preserves relative ordering of inputs

**Numerical Stability and Optimization**: Softmax is often implemented with a "max trick" to prevent overflow. There is also an \"online\" version that allows for carrying out the computation with fewer memory accesses, resulting in faster performance.`;

        const pos_encoding_arrows_info = "Apply positional encoding to embedded input $$x_{embd}$$ via addition (only if using the original positional encoding schema, else add $$0$$ to $$x_{embd}$$)";
        const addition_info = "Addition, if using the original positional encoding proposed but the original \"Attention is All You Need\" paper. Otherwise you can think of this as just a pass through connection for $$x_{embd}$$, or addition with $$0$$."

        const est_prob_dist_info = `The estimated probability distribution $$\\hat{P}( x_{tok}^{(t+1)} | x_{tok}^{(...t)} )$$ represents the model's predicted probability distribution for the next token given the previous tokens in the sequence.

**Shape**: A vector of length $$\\text{vocabsize}$$, where each element corresponds to a token in the vocabulary.

**Interpretation**: The value at vector index $$i$$ represents the model's estimated probability that the $$i$$-th token in the vocabulary will be the next token in the sequence.

**Sampling**: During text generation, the next token is typically sampled from this distribution, allowing for controlled randomness in the output.`;

        const sampling_info = `Sampling is the process of selecting the next token based on the probability distribution output by the model.

**Examples of sampling:**
1. **Top-k sampling**: Only consider the $$k$$ most likely next tokens.
   - Example: If $$k = 5$$, only the $$5$$ tokens with the highest probabilities are considered.
   - Process: Sort tokens by probability; Keep only the top k tokens; Renormalize probabilities; Sample from this reduced set.

2. **Top-p (nucleus) sampling**: Consider the smallest set of tokens whose cumulative probability exceeds the probability p.
   - Example: If $$p = 0.9$$, select the smallest set of tokens that sum to 90% probability.
   - Process: Sort tokens by probability; Add tokens to the set until cumulative probability > p; Renormalize probabilities; Sample from this dynamic set.

**Comparison:**
- Top-$$k$$ uses a fixed number of tokens, while Top-$$p$$ adapts based on the probability distribution.
- Top-$$p$$ can be more flexible, especially when the model is very confident or very uncertain.

Sampling strategies balance between diversity and quality of generated text. The choice of sampling method and its parameters can significantly affect the output, especially for longer sequences. Combining methods (e.g., Top-$$p$$ with a high-$$k$$ cutoff) can yield good results.`;

        const x_tok_t_plus_1_info = `$$x_{tok}^{(t+1)}$$ represents the next token in the sequence, sampled from the probability distribution output by the model.

**Sampling process**: This token is chosen based on the probability distribution $$\\hat{P}( x_{tok}^{(t+1)} | x_{tok}^{(...t)} )$$ using a sampling strategy (e.g., greedy, top-k, top-p).

**Token ID**: It's an integer corresponding to a token in the model's vocabulary.

**Auto-regressive nature**: 
   - In training: The model learns to predict $$x_{tok}^{(t+1)}$$ given $$x_{tok}^{(...t)}$$. This next token is known from the training data.
   - In inference: The predicted token becomes part of the input for the next iteration.

**Stopping criteria**: Generation typically continues until a stopping condition is met (e.g., max length, special end token).`;

        // Repeated Self-Attention info
        const x_embd_n_info = `$$x_{embd}^{(n)}$$ represents the embedded input data at the $$n$$th attention block of the transformer model.

**Content**: It contains the learned representations of the input tokens, updated by the previous layers of the transformer.

**Layer progression**: As $$n$$ increases, $$x_{embd}^{(n)}$$ captures increasingly abstract and context-aware representations of the input.

**Role in the transformer:**
- Carries forward the learned representations through the depth of the model.
- Allows for the gradual refinement of token representations based on their context within the sequence.

The quality and richness of these embeddings at each layer are crucial for the model's overall performance in understanding and generating text.`;

        const layernorm_info = `Layer Normalization (LayerNorm) is a technique used in transformer architectures to stabilize the hidden state dynamics in deep networks, reducing internal covariate shift and accelerating training by allowing higher learning rates.

**How LayerNorm works in transformers**: 
1. In a transformer, each token in a sequence is represented by a vector of features.
2. LayerNorm is applied independently to each token's feature vector: Calculate the mean and variance across all features of the token, normalize the features using these statistics, and apply learnable scale $$\\gamma$$ and shift $$\\beta$$ parameters.
3. This process is repeated in parallel for every token in the sequence and for every sequence in the batch.

**Formula**: For a token's feature vector $$x$$, LayerNorm is defined as:
   $$\\text{LayerNorm}(x) = \\dfrac{x - E[x]}{(\\text{Var}[x] + \\epsilon)^{1/2}} \\cdot \\gamma + \\beta$$
   where $$E[x]$$ is the mean and $$\\text{Var}[x]$$ is the variance, $$\\gamma$$ and $$\\beta$$ are learnable parameters, and $$\\epsilon$$ is a small constant for numerical stability.

**Variants**: Some models use variations like RMSNorm (Root Mean Square Layer Normalization) which simplifies the standard LayerNorm by removing the mean-centering step.`;

        const x_embd_n_tilde_info = `$$\\tilde{x}_{embd}^{(n)}$$ represents the normalized input embeddings at the $$n$$th self-attention block of the transformer model, after applying Layer Normalization.

**Characteristics**:
- Normalized: Each feature dimension has approximately zero mean and unit variance across the batch.
- Scale and shift: The $$\\gamma$$ and $$\\beta$$ parameters or LayerNorm allow the network to undo the normalization if needed.

**Impact**: LayerNorm helps the model train faster and generalizes better by ensuring that the scale of activations remains roughly consistent across different layers and during training.`;

        const dot_w_info = `The dot product operation with weight matrices $$W_Q^{(n)}$$, $$W_K^{(n)}$$, and $$W_V^{(n)}$$ transforms the input embeddings into query, key, and value representations respectively.

**Weight Matrices:**
1. $$W_Q^{(n)}$$: Query weight matrix
2. $$W_K^{(n)}$$: Key weight matrix
3. $$W_V^{(n)}$$: Value weight matrix

**Operations:**
- $$Q^{(n)} = x_{embd}^{(n)} \\cdot W_Q^{(n)}$$
- $$K^{(n)} = x_{embd}^{(n)} \\cdot W_K^{(n)}$$
- $$V^{(n)} = x_{embd}^{(n)} \\cdot W_V^{(n)}$$

These transformations allow the model to project the input into different subspaces, enabling it to attend to different aspects of the input when computing self-attention.

The superscript $$(n)$$ indicates that these are for the $$n$$-th attention layer, as each layer has its own set of learned weight matrices.`;

        const q_flat_info = `$$Q^{(n)}$$ represents the Query matrix for the $$n$$-th attention layer. It is derived from the input embeddings and is used to compute attention scores.

**Computation**: $$Q^{(n)} = x_{embd}^{(n)} \\cdot W_Q^{(n)}$$, where $$W_Q^{(n)}$$ is the learned weight matrix for queries.

**Role in attention**: Each row of $$Q^{(n)}$$ represents a query vector for a token in the sequence. These query vectors are used to compute how much attention each token should pay to other tokens in the sequence.

**Multi-head attention**: In practice, $$Q^{(n)}$$ is often split into multiple independent heads, each focusing on different aspects of the input.`;

        const k_flat_info = `$$K^{(n)}$$ represents the Key matrix for the $$n$$-th attention layer. It is derived from the input embeddings and is used to compute attention scores.

**Computation**: $$K^{(n)} = x_{embd}^{(n)} \\cdot W_K^{(n)}$$, where $$W_K^{(n)}$$ is the learned weight matrix for keys.

**Role in attention**: Each row of $$K^{(n)}$$ represents a key vector for a token in the sequence. These key vectors are used to determine how relevant each token is to the queries.

**Multi-head attention**: Like $$Q^{(n)}$$, $$K^{(n)}$$ is often split into multiple independent heads for parallel processing.

**Optimization Note:** During inference, key $$K^{(n)}$$ and value $$V^{(n)}$$ matrices are often cached and reused for subsequent tokens in a process called KV caching. This optimization significantly speeds up text generation by avoiding redundant computations.`;

        const v_flat_info = `$$V^{(n)}$$ represents the Value matrix for the $$n$$-th attention layer. It is derived from the input embeddings and contains the actual content that will be aggregated by the attention mechanism.

**Computation**: $$V^{(n)} = x_{embd}^{(n)} \\cdot W_V^{(n)}$$, where $$W_V^{(n)}$$ is the learned weight matrix for values.

**Role in attention**: Each row of $$V^{(n)}$$ represents a value vector for a token in the sequence. These value vectors contain the information that will be weighted and summed based on the attention scores.

**Multi-head attention**: Like $$Q^{(n)}$$ and $$K^{(n)}$$, $$V^{(n)}$$ is often split into multiple independent heads for parallel processing.

The attention mechanism uses the reshaped tensors $$Q'^{(n)}$$, $$K'^{(n)}$$, and $$V'^{(n)}$$ together to compute weighted sums of the values, where the weights are determined by the compatibility between queries and keys.

**Optimization Note:** During inference, $$K^{(n)}$$ and $$V^{(n)}$$ are often cached and reused for subsequent tokens in a process called KV caching. This optimization significantly speeds up text generation by avoiding redundant computations.`;

        const reshape_and_transpose_info = `We perform reshape and transpose operations to prepare the Query $$(Q')$$, Key $$(K')$$, and Value $$(V')$$ tensors for multi-head attention, shaping data in a format that's optimized for the multi-head attention mechanism, allowing each head to operate independently and in parallel.

**Reshape:**
First, we reshape the matrices from $$(\\text{seqlen} \\times d_{embd})$$ to tensors of shape $$(\\text{seqlen} \\times \\text{nheads} \\times \\text{headsize})$$, where: $$\\text{seqlen}$$ is the sequence length, $$\\text{nheads}$$ is the number of attention heads, and $$\\text{headsize}$$ is $$d_{embd} / \\text{nheads}$$.
This reshaping splits the embedding dimension into separate heads, allowing each head to focus on different aspects of the input.

**Transpose:**
After reshaping, we transpose the tensors to $$(\\text{nheads} \\times \\text{seqlen} \\times \\text{headsize})$$. By moving the $$\\text{nheads}$$ dimension to the front, we can treat each head as a separate item in a batch. In the subsequent steps, we need to perform tensor multiplications between $$Q'$$, $$K'$$, and $$V'$$. The transposition ensures that these tensors are in the correct shape for these operations.`;

        const q_reshaped_info = `$$Q'^{(n)}$$ represents the reshaped and transposed Query tensor for the $$n$$-th attention layer, prepared for multi-head attention.

**Purpose**: This transformed tensor enables parallel processing of multiple attention heads, each focusing on different aspects of the input sequence.

**Multi-head Attention**: By splitting the embedding dimension into multiple heads, the model can capture diverse relationships and interactions within the input sequence.`;

        const k_reshaped_info = `$$K'^{(n)}$$ represents the reshaped and transposed Key tensor for the $$n$$-th attention layer, prepared for multi-head attention.

**Purpose**: This transformed tensor enables parallel processing of multiple attention heads, each focusing on different aspects of the input sequence.

**Multi-head Attention**: By splitting the embedding dimension into multiple heads, the model can capture diverse relationships and interactions within the input sequence.`;

        const v_reshaped_info = `$$V'^{(n)}$$ represents the reshaped and transposed Value tensor for the $$n$$-th attention layer, prepared for multi-head attention.

**Purpose**: This transformed tensor enables parallel processing of multiple attention heads, each focusing on different aspects of the input sequence.

**Multi-head Attention**: By splitting the embedding dimension into multiple heads, the model can capture diverse relationships and interactions within the input sequence.`;

        const qk_dot_product_info = `The dot product of the Query tensor $$Q'^{(n)}$$ and the transposed Key tensor $$(K'^{(n)})^T$$ results in the affinities tensor is then used to compute the attention weights, which are applied to the Value tensor $$V'^{(n)}$$ to produce the self-attention output.

**Computation**:
$$\\text{affinities (not yet normalized)} = Q'^{(n)} \\cdot (K'^{(n)})^T$$

**Shape**: 
- $$Q'^{(n)}$$ has shape $$(\\text{nheads} \\times \\text{seqlen} \\times \\text{headsize})$$
- $$(K'^{(n)})^T$$ has shape $$(\\text{nheads} \\times \\text{headsize} \\times \\text{seqlen})$$
- The resulting affinities tensor has shape $$(\\text{nheads} \\times \\text{seqlen} \\times \\text{seqlen})$$

**Purpose**: 
- The dot product computes the similarity (or affinity) between each query and all keys.
- These affinities determine how much attention each token should pay to every other token in the sequence.`;

        const affinities_info = `The affinities tensor represents the raw attention scores between each pair of tokens in the sequence, computed as the dot product of the Query and Key tensors.

**Computation**: $$\\text{affinities} = \\frac{Q'^{(n)} \\cdot ({K'}^{(n)})^T}{\\text{headsize}^{1/2}}$$

**Purpose**: The dot product computes the similarity (or affinity) between each query and all keys. These affinities determine how much attention each token should pay to every other token in the sequence.

**Normalization with** $$\\text{headsize}^{1/2}$$: 
- The affinities are typically scaled by the square root of the head size, $$\\text{headsize}^{1/2}$$.
- **Why** $$\\text{sqrt(headsize)}$$?: When computing the dot product of two vectors, the expected magnitude of the result grows with the dimensionality of the vectors. Specifically, if the elements of the vectors are independent and identically distributed with mean $$0$$ and variance $$1$$, the dot product of two such vectors of dimension $$\(d\)$$ will have a variance of $$\(d\)$$. Therefore, to keep the scale of the dot product independent of the dimensionality, we divide by the square root of the head size, which is the dimensionality of the vectors in this context. This scaling ensures that the values in the affinities tensor remain in a manageable range, leading to more stable gradients and better training dynamics.`;

        const mask_and_normalize_info = `The mask and normalize step ensures that the model attends to the appropriate tokens and that the attention scores are properly scaled.

**Masking**:
1. **Purpose**: Prevents the model from attending to certain positions, such as future tokens in a sequence (causal masking) or padding tokens.

2. **Implementation**: Typically involves adding a large negative value (e.g., $$-\\infty$$) to the attention scores of masked positions before applying the softmax function, effectively setting their attention weights to zero.

**Normalization**:
1. **Purpose**: Converts the raw attention scores into a probability distribution, ensuring that the sum of attention weights for each token is 1.

2. **Softmax Function**: Applied to the masked attention scores to achieve normalization.`;

        const masked_affinities_info = `The masked affinities tensor represents the attention scores after applying a mask to prevent the model from attending to certain positions, such as future tokens in a sequence (causal masking) or padding tokens.

**Purpose**: 
1. **Causal Masking**: Ensures that the model only attends to previous tokens in the sequence, preventing information leakage from future tokens during training.
2. **Padding Masking**: Prevents the model from attending to padding tokens, which are used to ensure that all sequences in a batch have the same length.

**Implementation**: 
A large negative value (e.g., $$-\\infty$$) is added to the attention scores of masked positions before applying the softmax function. This effectively sets their attention weights to zero.

**Shape**: The masked affinities tensor has the same shape as the affinities tensor: $$(\\text{nheads} \\times \\text{seqlen} \\times \\text{seqlen})$$.

**Normalization**: After masking, the softmax function is applied to convert the masked affinities into a probability distribution, ensuring that the sum of attention weights for each token is $$1$$.`;

        const sa_dot_product_info = `The self-attention dot product operation combines the attention weights with the Value tensor to produce the final self-attention output.

**Computation**: $$ \\text{softmax}(\\frac{Q'^{(n)} \\cdot (K'^{(n)})^T}{\\text{headsize}^{1/2}}) \\cdot V'^{(n)}$$

**Steps**:
1. **Dot Product of Queries and Keys**: Compute the raw attention scores by taking the dot product of the Query tensor $$Q'^{(n)}$$ and the transposed Key tensor $$(K'^{(n)})^T$$.

2. **Scaling**: Scale the raw attention scores by the square root of head size, $$\\text{headsize}^{1/2}$$, to maintain stable gradients.

3. **Softmax**: Apply the softmax function to the scaled attention scores to obtain the attention weights, ensuring they sum to $$1$$.

4. **Weighted Sum**: Multiply the attention weights by the Value tensor $$V'^{(n)}$$ to get the final self-attention output.`;

        const sa_out_info = `This self-attention output represents the result of applying the attention mechanism to the input embeddings.

**Computation**: $$\\text{softmax}(\\frac{Q'^{(n)} \\cdot (K'^{(n)})^T}{\\text{headsize}^{1/2}}) \\cdot V'^{(n)}$$

**Key points**:
1. Each token's representation now contains information from other relevant tokens in the sequence.
2. The output maintains the same dimensionality as the input, allowing for stacking of multiple attention layers.
3. Different attention heads may focus on different aspects of the relationships between tokens.

This output captures complex dependencies and relationships within the input sequence, allowing the model to process context-dependent information effectively.

**Optimization Note**: In practice, this value is computed by combining the calculation of the affinities matrix, masking, normalization, and dot product with $$V'^{(n)}$$ into a kernel fusion operation, known as FlashAttention.`;

        const reshape_sa_out_info = `The reshape operation on the self-attention output prepares the output for further processing in the transformer architecture.

**Purpose**: To combine the information from all attention heads and match the original input embedding dimension.

**Process**:
1. Concatenate the outputs from all attention heads.
2. Reshape the concatenated output to match the input embedding dimension.

**Shape transformation**:
From: $$(\\text{nheads} \\times \\text{seqlen} \\times \\text{headsize})$$
To: $$(\\text{seqlen} \\times d_{embd})$$, where $$d_{embd} = \\text{nheads} * \\text{headsize}$$

This reshaping allows the model to integrate information from all attention heads while maintaining a consistent tensor shape throughout the network.`;

        const sa_out_reshaped_info = `$$out_{SA}^{(n)}$$ represents the reshaped output of the self-attention mechanism, ready for further processing in the transformer layer.

**Key aspects**:
1. Combines information from all attention heads into a single representation for each token.
2. Maintains the sequence length dimension, preserving the order of tokens.
3. Matches the dimensionality of the input embeddings, allowing for residual connections and further layer processing.

This reshaped output encapsulates the contextual information gathered by the self-attention mechanism, allowing subsequent layers to further process and refine the token representations.`;

        const residual_connection_info = `Residual connections, also known as skip connections, are a key feature in many deep neural network architectures, including transformers. They allow the network to bypass one or more layers, providing a direct route for information and gradients to flow through the network.

**Key aspects of residual connections:**
1. **Formula**: For a given layer with input $$x$$ and output $$F(x)$$, the residual connection adds the input to the output: 
$$y = F(x) + x$$
2. **Gradient flow**: They help mitigate the vanishing gradient problem by providing a direct path for gradients to flow backward through the network.
3. **Information preservation**: Allow the network to easily learn identity mappings, preserving important features from earlier layers.
4. **Deeper networks**: Enable the training of much deeper networks by addressing degradation problems that occur in very deep architectures.

Residual connections are crucial for the performance and trainability of deep transformer models, allowing them to effectively process and integrate information across many layers.`;

        const add_residual_info = "Addition to connect the residual path with the main path. This allows information to flow directly from earlier layers, helping with gradient flow and preserving important features.";
        const layernorm_ff_MLP_info = `This step represents the Layer Normalization (LayerNorm) followed by the Feed-Forward Multi-Layer Perceptron (FF MLP) in a transformer block.

Common choices for nonlinearity include ReLU (original attention paper), GELU (GPT2), and SiLU (Llama 3)`;

        const hidden_layer_MLP_info = `The hidden layer in the Multi-Layer Perceptron (MLP) part of the transformer block performs non-linear transformations on the input data.

**Key aspects:**
1. **Dimensionality**: Often larger than the input dimension (e.g., 4 times larger).

2. **Activation Function**: Typically uses non-linear activations like ReLU, GELU, or SiLU.

3. **Purpose**: Allows the model to learn complex, non-linear relationships in the data.`;

        const ff_MLP_info = `The second linear transformation in the feedforward Multi-Layer Perceptron (MLP) projects the data back to the original embedding dimension.

**Purpose**: Maps the expanded hidden layer back to the model's embedding dimension. Reduces the dimensionality from the expanded hidden layer (e.g., $$ 4 d_{embd}$$) back to $$d_{embd}$$.

This transformation, combined with the preceding non-linear activation, enables the feedforward MLP to introduce non-linearity and increase the model's overall expressiveness.`;

        const out_MLP_info = `$$out_{MLP}^{(n)}$$ represents the output of the Multi-Layer Perceptron (MLP) in the $$n$$-th transformer block.

**Key aspects:**
1. **Shape**: Maintains the shape $$(\\text{seqlen} \\times d_{embd})$$, matching the input dimension.

2. **Function**: Increases the model's capacity to learn complex patterns.

3. **Intuition**: Represents the output of a "computation" step after the self-attention "communication" step. Alternates with self-attention layers, creating a pattern of "communicate, then compute" that is repeated throughout the network.

4. **Residual connection**: The MLP output is typically added to its input via a residual connection, helping with gradient flow in deep networks.

The MLP layer, alternating with self-attention layers, forms the core of the transformer architecture, allowing the model to capture both local and global dependencies in the input sequence.`;

        const x_emb_n_plus_one_info = `$$x_{embd}^{(n+1)}$$ represents the embedded input for the next multi-headed self-attention block (or the final output if this is the last block) in the transformer model.`;

        const svgContent = {
            'successive-self-attention': {
                // 'root': {
                boxes: [
                    {
                        x: 20, y: 340, text: '', width: SINGLE_VECTOR_WIDTH * 4, height: SINGLE_VECTOR_HEIGHT / 2,
                        info: x_embd_n_info, opacity: 0.4
                    },
                    {
                        x: 20 + MIN_SEP_HORIZONTAL, y: 340, text: '', width: SINGLE_VECTOR_WIDTH * 4, height: SINGLE_VECTOR_HEIGHT / 2,
                        info: x_embd_n_tilde_info, opacity: 0.4
                    },


                    {
                        x: 70 + 2 * MIN_SEP_HORIZONTAL, y: 40, text: '', width: SINGLE_VECTOR_WIDTH * 4, height: SINGLE_VECTOR_HEIGHT / 2,
                        info: q_flat_info, opacity: 0.4
                    },
                    {
                        x: 70 + 2 * MIN_SEP_HORIZONTAL, y: 340, text: '', width: SINGLE_VECTOR_WIDTH * 4, height: SINGLE_VECTOR_HEIGHT / 2,
                        info: k_flat_info, opacity: 0.4
                    },
                    {
                        x: 70 + 2 * MIN_SEP_HORIZONTAL, y: 640, text: '', width: SINGLE_VECTOR_WIDTH * 4, height: SINGLE_VECTOR_HEIGHT / 2,
                        info: v_flat_info, opacity: 0.4
                    },

                    // Q K V resized
                    {
                        x: 70 + 3 * MIN_SEP_HORIZONTAL, y: 40, text: '', width: SINGLE_VECTOR_WIDTH * 4 / 3, height: SINGLE_VECTOR_HEIGHT / 2,
                        info: q_reshaped_info, opacity: 0.4
                    },
                    {
                        x: 95 + 3 * MIN_SEP_HORIZONTAL, y: 25, text: '', width: SINGLE_VECTOR_WIDTH * 4 / 3, height: SINGLE_VECTOR_HEIGHT / 2,
                        info: q_reshaped_info, opacity: 0.4
                    },
                    {
                        x: 120 + 3 * MIN_SEP_HORIZONTAL, y: 10, text: '', width: SINGLE_VECTOR_WIDTH * 4 / 3, height: SINGLE_VECTOR_HEIGHT / 2,
                        info: q_reshaped_info, opacity: 0.4
                    },

                    {
                        x: 70 + 3 * MIN_SEP_HORIZONTAL, y: 340, text: '', width: SINGLE_VECTOR_WIDTH * 4 / 3, height: SINGLE_VECTOR_HEIGHT / 2,
                        info: k_reshaped_info, opacity: 0.4
                    },
                    {
                        x: 95 + 3 * MIN_SEP_HORIZONTAL, y: 325, text: '', width: SINGLE_VECTOR_WIDTH * 4 / 3, height: SINGLE_VECTOR_HEIGHT / 2,
                        info: k_reshaped_info, opacity: 0.4
                    },
                    {
                        x: 120 + 3 * MIN_SEP_HORIZONTAL, y: 310, text: '', width: SINGLE_VECTOR_WIDTH * 4 / 3, height: SINGLE_VECTOR_HEIGHT / 2,
                        info: k_reshaped_info, opacity: 0.4
                    },

                    {
                        x: 70 + 3 * MIN_SEP_HORIZONTAL, y: 640, text: '', width: SINGLE_VECTOR_WIDTH * 4 / 3, height: SINGLE_VECTOR_HEIGHT / 2,
                        info: v_reshaped_info, opacity: 0.4
                    },
                    {
                        x: 95 + 3 * MIN_SEP_HORIZONTAL, y: 625, text: '', width: SINGLE_VECTOR_WIDTH * 4 / 3, height: SINGLE_VECTOR_HEIGHT / 2,
                        info: v_reshaped_info, opacity: 0.4
                    },
                    {
                        x: 120 + 3 * MIN_SEP_HORIZONTAL, y: 610, text: '', width: SINGLE_VECTOR_WIDTH * 4 / 3, height: SINGLE_VECTOR_HEIGHT / 2,
                        info: v_reshaped_info, opacity: 0.4
                    },

                    // Affinities
                    {
                        x: 200 + 4 * MIN_SEP_HORIZONTAL, y: 190, text: '', width: SINGLE_VECTOR_HEIGHT / 2, height: SINGLE_VECTOR_HEIGHT / 2,
                        info: affinities_info, opacity: 0.4
                    },
                    {
                        x: 225 + 4 * MIN_SEP_HORIZONTAL, y: 175, text: '', width: SINGLE_VECTOR_HEIGHT / 2, height: SINGLE_VECTOR_HEIGHT / 2,
                        info: affinities_info, opacity: 0.4
                    },
                    {
                        x: 250 + 4 * MIN_SEP_HORIZONTAL, y: 160, text: '', width: SINGLE_VECTOR_HEIGHT / 2, height: SINGLE_VECTOR_HEIGHT / 2,
                        info: affinities_info, opacity: 0.4
                    },

                    // Self attention output
                    {
                        x: 1750, y: 340, text: '', width: SINGLE_VECTOR_WIDTH * 4 / 3, height: SINGLE_VECTOR_HEIGHT / 2,
                        info: sa_out_info, opacity: 0.4
                    },
                    {
                        x: 1775, y: 325, text: '', width: SINGLE_VECTOR_WIDTH * 4 / 3, height: SINGLE_VECTOR_HEIGHT / 2,
                        info: sa_out_info, opacity: 0.4
                    },
                    {
                        x: 1800, y: 310, text: '', width: SINGLE_VECTOR_WIDTH * 4 / 3, height: SINGLE_VECTOR_HEIGHT / 2,
                        info: sa_out_info, opacity: 0.4
                    },
                    {
                        x: 1950, y: 340, text: '', width: SINGLE_VECTOR_WIDTH * 4, height: SINGLE_VECTOR_HEIGHT / 2,
                        info: sa_out_reshaped_info, opacity: 0.4
                    },

                    {
                        x: 2150 - SINGLE_LATEX_CHAR_WIDTH / 2, y: 405,
                        text: '',
                        opacity: 0.2,
                        width: SINGLE_LATEX_CHAR_WIDTH,
                        height: SINGLE_LATEX_CHAR_HEIGHT,
                        info: add_residual_info
                    },
                    {
                        x: 2300,
                        y: 390,
                        text: 'Hidden Layer',
                        width: SINGLE_VECTOR_WIDTH * 4, height: SINGLE_VECTOR_WIDTH * 4,
                        info: hidden_layer_MLP_info, opacity: 0.4
                    },
                    {
                        x: 2500, y: 340, text: '', width: SINGLE_VECTOR_WIDTH * 4, height: SINGLE_VECTOR_HEIGHT / 2,
                        info: out_MLP_info, opacity: 0.4
                    },
                    {
                        x: 2700 - SINGLE_LATEX_CHAR_WIDTH / 2, y: 405,
                        text: '',
                        opacity: 0.2,
                        width: SINGLE_LATEX_CHAR_WIDTH,
                        height: SINGLE_LATEX_CHAR_HEIGHT,
                        info: add_residual_info
                    },
                ],
                triangles: [
                    {
                        x1: 1400, y1: 190,
                        x2: 1400, y2: 390,
                        x3: 1600, y3: 390,
                        color: RECT_FILL_COLOR,
                        opacity: 0.4,
                        info: masked_affinities_info,
                    },
                    {
                        x1: 1425, y1: 175,
                        x2: 1425, y2: 375,
                        x3: 1625, y3: 375,
                        color: RECT_FILL_COLOR,
                        opacity: 0.4,
                        info: masked_affinities_info,
                    },
                    {
                        x1: 1450, y1: 160,
                        x2: 1450, y2: 360,
                        x3: 1650, y3: 360,
                        color: RECT_FILL_COLOR,
                        opacity: 0.4,
                        info: masked_affinities_info,
                    },
                    // ... more triangles
                ],

                latexLabels: [
                    // x embed
                    {
                        x: 20 + (SINGLE_VECTOR_WIDTH * 2), y: 430,
                        text: 'x_{embd}^{(n)}',
                        color: LATEX_LABEL_COLOR, width: SINGLE_LATEX_CHAR_WIDTH, height: SINGLE_LATEX_CHAR_HEIGHT,
                        info: x_embd_n_info
                    },
                    {
                        x: 20 + MIN_SEP_HORIZONTAL + (SINGLE_VECTOR_WIDTH * 2), y: 430,
                        text: '\\tilde{x}_{embd}^{(n)}',
                        color: LATEX_LABEL_COLOR, width: SINGLE_LATEX_CHAR_WIDTH, height: SINGLE_LATEX_CHAR_HEIGHT,
                        info: x_embd_n_tilde_info,
                    },
                    {
                        x: 20 + (SINGLE_VECTOR_WIDTH * 2), y: 310,
                        text: '(\\text{seqlen} \\times d_{embd})',
                        color: LATEX_SHAPE_LABEL_COLOR, width: SINGLE_LATEX_CHAR_WIDTH * 3, height: SINGLE_LATEX_CHAR_HEIGHT,
                        info: x_embd_n_info
                    },
                    {
                        x: 20 + MIN_SEP_HORIZONTAL + (SINGLE_VECTOR_WIDTH * 2), y: 310,
                        text: '(\\text{seqlen} \\times d_{embd})',
                        color: LATEX_SHAPE_LABEL_COLOR, width: SINGLE_LATEX_CHAR_WIDTH * 3, height: SINGLE_LATEX_CHAR_HEIGHT,
                        info: x_embd_n_tilde_info,
                    },
                    // Q K V flat
                    {
                        x: 120 + 2 * MIN_SEP_HORIZONTAL, y: 130,
                        text: 'Q^{(n)}',
                        color: LATEX_LABEL_COLOR, width: SINGLE_LATEX_CHAR_WIDTH, height: SINGLE_LATEX_CHAR_HEIGHT,
                        info: q_flat_info,
                    },
                    {
                        x: 120 + 2 * MIN_SEP_HORIZONTAL, y: 430,
                        text: 'K^{(n)}',
                        color: LATEX_LABEL_COLOR, width: SINGLE_LATEX_CHAR_WIDTH, height: SINGLE_LATEX_CHAR_HEIGHT,
                        info: k_flat_info,
                    },
                    {
                        x: 120 + 2 * MIN_SEP_HORIZONTAL, y: 730,
                        text: 'V^{(n)}',
                        color: LATEX_LABEL_COLOR, width: SINGLE_LATEX_CHAR_WIDTH, height: SINGLE_LATEX_CHAR_HEIGHT,
                        info: v_flat_info,
                    },

                    {
                        x: 120 + 2 * MIN_SEP_HORIZONTAL, y: 10,
                        text: '(\\text{seqlen} \\times d_{embd})',
                        color: LATEX_SHAPE_LABEL_COLOR, width: SINGLE_LATEX_CHAR_WIDTH * 3, height: SINGLE_LATEX_CHAR_HEIGHT,
                        info: q_flat_info
                    },
                    {
                        x: 120 + 2 * MIN_SEP_HORIZONTAL, y: 310,
                        text: '(\\text{seqlen} \\times d_{embd})',
                        color: LATEX_SHAPE_LABEL_COLOR, width: SINGLE_LATEX_CHAR_WIDTH * 3, height: SINGLE_LATEX_CHAR_HEIGHT,
                        info: k_flat_info,
                    },
                    {
                        x: 120 + 2 * MIN_SEP_HORIZONTAL, y: 610,
                        text: '(\\text{seqlen} \\times d_{embd})',
                        color: LATEX_SHAPE_LABEL_COLOR, width: SINGLE_LATEX_CHAR_WIDTH * 3, height: SINGLE_LATEX_CHAR_HEIGHT,
                        info: v_flat_info,
                    },

                    // Q K V prime
                    {
                        x: 120 + 3 * MIN_SEP_HORIZONTAL, y: 130,
                        text: 'Q\'^{(n)}',
                        color: LATEX_LABEL_COLOR, width: SINGLE_LATEX_CHAR_WIDTH, height: SINGLE_LATEX_CHAR_HEIGHT,
                        info: q_reshaped_info,
                    },
                    {
                        x: 120 + 3 * MIN_SEP_HORIZONTAL, y: -20,
                        text: '(\\text{nheads} \\times \\text{seqlen} \\times \\text{headsize})',
                        color: LATEX_SHAPE_LABEL_COLOR, width: SINGLE_LATEX_CHAR_WIDTH * 6, height: SINGLE_LATEX_CHAR_HEIGHT,
                        info: q_reshaped_info
                    },
                    {
                        x: 120 + 3 * MIN_SEP_HORIZONTAL, y: 430,
                        text: 'K\'^{(n)}',
                        color: LATEX_LABEL_COLOR, width: SINGLE_LATEX_CHAR_WIDTH, height: SINGLE_LATEX_CHAR_HEIGHT,
                        info: k_reshaped_info,
                    },
                    {
                        x: 120 + 3 * MIN_SEP_HORIZONTAL, y: 280,
                        text: '(\\text{nheads} \\times \\text{seqlen} \\times \\text{headsize})',
                        color: LATEX_SHAPE_LABEL_COLOR, width: SINGLE_LATEX_CHAR_WIDTH * 6, height: SINGLE_LATEX_CHAR_HEIGHT,
                        info: k_reshaped_info
                    },
                    {
                        x: 120 + 3 * MIN_SEP_HORIZONTAL, y: 730,
                        text: 'V\'^{(n)}',
                        color: LATEX_LABEL_COLOR, width: SINGLE_LATEX_CHAR_WIDTH, height: SINGLE_LATEX_CHAR_HEIGHT,
                        info: v_reshaped_info,
                    },
                    {
                        x: 120 + 3 * MIN_SEP_HORIZONTAL, y: 580,
                        text: '(\\text{nheads} \\times \\text{seqlen} \\times \\text{headsize})',
                        color: LATEX_SHAPE_LABEL_COLOR, width: SINGLE_LATEX_CHAR_WIDTH * 6, height: SINGLE_LATEX_CHAR_HEIGHT,
                        info: v_reshaped_info
                    },

                    // dot W_QKV
                    {
                        x: 2 * MIN_SEP_HORIZONTAL - 10, y: 110,
                        // text: '\\_ \\cdot W_{K}^{(n)}',
                        text: '\\cdot \\ W_{Q}^{(n)}',
                        color: LATEX_LABEL_SECONDARY_COLOR, width: SINGLE_LATEX_CHAR_WIDTH * 2, height: SINGLE_LATEX_CHAR_HEIGHT,
                        info: dot_w_info,
                    },
                    {
                        x: 2 * MIN_SEP_HORIZONTAL - 10, y: 410,
                        // text: '\\_ \\cdot W_{K}^{(n)}',
                        text: '\\cdot \\ W_{K}^{(n)}',
                        color: LATEX_LABEL_SECONDARY_COLOR, width: SINGLE_LATEX_CHAR_WIDTH * 2, height: SINGLE_LATEX_CHAR_HEIGHT,
                        info: dot_w_info,
                    },
                    {
                        x: 2 * MIN_SEP_HORIZONTAL - 10, y: 710,
                        // text: '\\_ \\cdot W_{K}^{(n)}',
                        text: '\\cdot \\ W_{V}^{(n)}',
                        color: LATEX_LABEL_SECONDARY_COLOR, width: SINGLE_LATEX_CHAR_WIDTH * 2, height: SINGLE_LATEX_CHAR_HEIGHT,
                        info: dot_w_info,
                    },
                    // {
                    //     x: 880, 
                    //     y: 290, 
                    //     text: 'Q'^{(n)} \\cdot (K'^{(n)})^\\mathbf{T}', 
                    //     color: LATEX_LABEL_COLOR,
                    //     info: layernorm_info, 
                    // },
                    {
                        x: 1120, y: 130,
                        text: '(\\text{nheads} \\times \\text{seqlen} \\times \\text{seqlen} )',
                        color: LATEX_SHAPE_LABEL_COLOR, width: SINGLE_LATEX_CHAR_WIDTH * 6, height: SINGLE_LATEX_CHAR_HEIGHT,
                        info: affinities_info
                    },
                    {
                        x: 1520, y: 130,
                        text: '(\\text{nheads} \\times \\text{seqlen} \\times \\text{seqlen} )',
                        color: LATEX_SHAPE_LABEL_COLOR, width: SINGLE_LATEX_CHAR_WIDTH * 6, height: SINGLE_LATEX_CHAR_HEIGHT,
                        info: masked_affinities_info
                    },

                    {
                        x: 1800, y: 280,
                        text: '(\\text{nheads} \\times \\text{seqlen} \\times \\text{headsize})',
                        color: LATEX_SHAPE_LABEL_COLOR, width: SINGLE_LATEX_CHAR_WIDTH * 6, height: SINGLE_LATEX_CHAR_HEIGHT,
                        info: sa_out_info
                    },
                    {
                        x: 2000, y: 310,
                        text: '(\\text{seqlen} \\times d_{embd})',
                        color: LATEX_SHAPE_LABEL_COLOR, width: SINGLE_LATEX_CHAR_WIDTH * 3, height: SINGLE_LATEX_CHAR_HEIGHT,
                        info: sa_out_reshaped_info,
                    },
                    {
                        x: 2000, y: 430,
                        text: '{out}_{SA}^{(n)}',
                        color: LATEX_LABEL_COLOR, width: 2 * SINGLE_LATEX_CHAR_WIDTH, height: SINGLE_LATEX_CHAR_HEIGHT,
                        info: sa_out_reshaped_info,
                    },
                    {
                        x: 2150, y: 422,
                        text: '\\boldsymbol{+}',
                        width: SINGLE_LATEX_CHAR_WIDTH, height: SINGLE_LATEX_CHAR_HEIGHT,
                        info: add_residual_info
                    },
                    {
                        x: 2550, y: 430,
                        text: '{out}_{MLP}^{(n)}',
                        color: LATEX_LABEL_COLOR, width: 2 * SINGLE_LATEX_CHAR_WIDTH, height: SINGLE_LATEX_CHAR_HEIGHT,
                        info: out_MLP_info,
                    },
                    {
                        x: 2550, y: 310,
                        text: '(\\text{seqlen} \\times d_{embd})',
                        color: LATEX_SHAPE_LABEL_COLOR, width: SINGLE_LATEX_CHAR_WIDTH * 3, height: SINGLE_LATEX_CHAR_HEIGHT,
                        info: out_MLP_info,
                    },
                    {
                        x: 2700, y: 422,
                        text: '\\boldsymbol{+}',
                        width: SINGLE_LATEX_CHAR_WIDTH, height: SINGLE_LATEX_CHAR_HEIGHT,
                        info: add_residual_info
                    },
                    {
                        x: 2780, y: 422,
                        text: '\\boldsymbol{=} x_{embd}^{(n+1)}',
                        width: SINGLE_LATEX_CHAR_WIDTH * 2, height: SINGLE_LATEX_CHAR_HEIGHT, info: x_emb_n_plus_one_info
                    },
                    // {
                    //     x: 1670, y: 422,
                    //     text: '\\boldsymbol{\\cdot}',
                    //     color: LATEX_LABEL_SECONDARY_COLOR,
                    //     width: SINGLE_LATEX_CHAR_WIDTH, height: SINGLE_LATEX_CHAR_HEIGHT, 
                    // },
                ],
                textLabels: [
                    {
                        x: 125,
                        y: 430,
                        text: 'LayerNorm',
                        color: LATEX_LABEL_SECONDARY_COLOR,
                        info: layernorm_info,
                    },
                    // reshape and transpose
                    {
                        x: 590,
                        y: 130,
                        text: 'reshape',
                        color: LATEX_SHAPE_LABEL_COLOR,
                        info: reshape_and_transpose_info,
                    },
                    {
                        x: 590,
                        y: 430,
                        text: 'reshape',
                        color: LATEX_SHAPE_LABEL_COLOR,
                        info: reshape_and_transpose_info,
                    },
                    {
                        x: 590,
                        y: 730,
                        text: 'reshape',
                        color: LATEX_SHAPE_LABEL_COLOR,
                        info: reshape_and_transpose_info,
                    },
                    // {
                    //     x: 590,
                    //     y: 160,
                    //     text: 'transpose',
                    //     color: LATEX_SHAPE_LABEL_COLOR,
                    //     info: reshape_and_transpose_info,
                    // },
                    // {
                    //     x: 590,
                    //     y: 460,
                    //     text: 'transpose',
                    //     color: LATEX_SHAPE_LABEL_COLOR,
                    //     info: reshape_and_transpose_info,
                    // },
                    // {
                    //     x: 590,
                    //     y: 760,
                    //     text: 'transpose',
                    //     color: LATEX_SHAPE_LABEL_COLOR,
                    //     info: reshape_and_transpose_info,
                    // },
                    {
                        x: 260 + 4 * MIN_SEP_HORIZONTAL,
                        y: 290,
                        text: 'Token Affinities',
                        color: LATEX_LABEL_COLOR,
                        info: affinities_info,
                    },
                    {
                        x: 220 + 6 * MIN_SEP_HORIZONTAL,
                        y: 290,
                        text: 'Self-Attention Weights',
                        color: LATEX_LABEL_COLOR,
                        info: masked_affinities_info,
                    },
                    {
                        x: 905,
                        y: 280,
                        text: 'dot product',
                        color: LATEX_LABEL_SECONDARY_COLOR,
                        info: qk_dot_product_info,
                    },
                    {
                        x: 780,
                        y: 430,
                        text: 'transpose',
                        color: LATEX_SHAPE_LABEL_COLOR,
                        info: qk_dot_product_info,
                    },
                    {
                        x: 1252,
                        y: 280,
                        text: 'mask and normalize',
                        color: LATEX_LABEL_SECONDARY_COLOR,
                        info: mask_and_normalize_info,
                    },
                    {
                        x: 1570,
                        y: 430,
                        text: 'dot product',
                        color: LATEX_LABEL_SECONDARY_COLOR,
                        info: sa_dot_product_info,
                    },
                    {
                        x: 1850,
                        y: 420,
                        text: 'reshape',
                        color: LATEX_SHAPE_LABEL_COLOR,
                        info: reshape_sa_out_info,
                    },
                    {
                        x: 80,
                        y: -80,
                        text: 'Residual Connection',
                        color: LATEX_SHAPE_LABEL_COLOR,
                        info: residual_connection_info,
                    },

                    {
                        x: 2185,
                        y: 450,
                        text: 'LayerNorm,',
                        color: LATEX_SHAPE_LABEL_COLOR,
                        info: layernorm_ff_MLP_info,
                    },
                    {
                        x: 2185,
                        y: 470,
                        text: 'FF, nonlinearity',
                        color: LATEX_SHAPE_LABEL_COLOR,
                        info: layernorm_ff_MLP_info,
                    },
                    {
                        x: 2210,
                        y: -80,
                        text: 'Residual Connection',
                        color: LATEX_SHAPE_LABEL_COLOR,
                        info: residual_connection_info,
                    },
                    {
                        x: 2405,
                        y: 450,
                        text: 'feedforward',
                        color: LATEX_SHAPE_LABEL_COLOR,
                        info: ff_MLP_info,
                    },
                ],
                arrows: [
                    {
                        start: { x: 20 + 4 * SINGLE_VECTOR_WIDTH, y: 440 },
                        end: { x: 20 + MIN_SEP_HORIZONTAL, y: 440 },
                        color: ARROW_COLOR,
                        info: layernorm_info,
                    },
                    // {
                    //     start: { x: 20 + 4*SINGLE_VECTOR_WIDTH + MIN_SEP_HORIZONTAL , y: 140 },
                    //     end: { x: 20 + 2*SINGLE_VECTOR_WIDTH + 2*MIN_SEP_HORIZONTAL, y: 140 },
                    //     color: ARROW_COLOR,
                    //     info: dot_w_q_info,
                    // },
                    {
                        start: { x: 20 + 4 * SINGLE_VECTOR_WIDTH + MIN_SEP_HORIZONTAL, y: 440 },
                        end: { x: 20 + 2 * SINGLE_VECTOR_WIDTH + 2 * MIN_SEP_HORIZONTAL, y: 440 },
                        color: ARROW_COLOR,
                        info: dot_w_info,
                    },
                    // {
                    //     start: { x: 20 + 4*SINGLE_VECTOR_WIDTH + MIN_SEP_HORIZONTAL , y: 740 },
                    //     end: { x: 20 + 2*SINGLE_VECTOR_WIDTH + 2*MIN_SEP_HORIZONTAL, y: 740 },
                    //     color: ARROW_COLOR,
                    //     info: dot_w_v_info,
                    // },
                    {
                        start: { x: 3 * MIN_SEP_HORIZONTAL - 34, y: 140 },
                        end: { x: 20 + 2 * SINGLE_VECTOR_WIDTH + 3 * MIN_SEP_HORIZONTAL, y: 140 },
                        color: ARROW_COLOR,
                        info: reshape_and_transpose_info,
                    },
                    {
                        start: { x: 3 * MIN_SEP_HORIZONTAL - 34, y: 440 },
                        end: { x: 20 + 2 * SINGLE_VECTOR_WIDTH + 3 * MIN_SEP_HORIZONTAL, y: 440 },
                        color: ARROW_COLOR,
                        info: reshape_and_transpose_info,
                    },
                    {
                        start: { x: 3 * MIN_SEP_HORIZONTAL - 34, y: 740 },
                        end: { x: 20 + 2 * SINGLE_VECTOR_WIDTH + 3 * MIN_SEP_HORIZONTAL, y: 740 },
                        color: ARROW_COLOR,
                        info: reshape_and_transpose_info,
                    },

                    // self attention
                    {
                        start: { x: 1250, y: 290 },
                        end: { x: 1400, y: 290 },
                        color: ARROW_COLOR,
                        info: mask_and_normalize_info,
                    },

                    {
                        start: { x: 1830, y: 430 },
                        end: { x: 1950, y: 430 },
                        color: ARROW_COLOR,
                        info: reshape_and_transpose_info,
                    },
                    {
                        start: { x: 2045, y: 430 },
                        end: { x: 2125, y: 430 },
                        color: ARROW_COLOR,
                        info: "Combine with residual connection",
                    },
                    {
                        start: { x: 2175, y: 430 },
                        end: { x: 2300, y: 430 },
                        color: ARROW_COLOR,
                        info: layernorm_ff_MLP_info,
                    },
                    {
                        start: { x: 2395, y: 430 },
                        end: { x: 2500, y: 430 },
                        color: ARROW_COLOR,
                        info: ff_MLP_info,
                    },
                    {
                        start: { x: 2595, y: 430 },
                        end: { x: 2675, y: 430 },
                        color: ARROW_COLOR,
                        info: "Combine with residual connection",
                    },
                ],
                anchoredArrows: [
                    {
                        // W_Q, W_V
                        segments: [
                            {
                                start: { x: 20 + MIN_SEP_HORIZONTAL + 2 * SINGLE_VECTOR_WIDTH, y: 340 },
                                end: { x: 20 + MIN_SEP_HORIZONTAL + 2 * SINGLE_VECTOR_WIDTH, y: 140 }
                            },
                            {
                                start: { x: 20 + MIN_SEP_HORIZONTAL + 2 * SINGLE_VECTOR_WIDTH, y: 140 },
                                end: { x: 20 + 2 * MIN_SEP_HORIZONTAL + 2 * SINGLE_VECTOR_WIDTH, y: 140 }
                            },
                        ],
                        color: ARROW_COLOR,
                        info: dot_w_info,
                    },
                    {
                        segments: [
                            {
                                start: { x: 20 + MIN_SEP_HORIZONTAL + 2 * SINGLE_VECTOR_WIDTH, y: 540 },
                                end: { x: 20 + MIN_SEP_HORIZONTAL + 2 * SINGLE_VECTOR_WIDTH, y: 740 }
                            },
                            {
                                start: { x: 20 + MIN_SEP_HORIZONTAL + 2 * SINGLE_VECTOR_WIDTH, y: 740 },
                                end: { x: 20 + 2 * MIN_SEP_HORIZONTAL + 2 * SINGLE_VECTOR_WIDTH, y: 740 }
                            },
                        ],
                        color: ARROW_COLOR,
                        info: dot_w_info,
                    },
                    // Token affinities transpose and dot product
                    {
                        segments: [
                            {
                                start: { x: 750, y: 140 },
                                end: { x: 900, y: 140 }
                            },
                            {
                                start: { x: 900, y: 140 },
                                end: { x: 900, y: 290 }
                            },
                            {
                                start: { x: 900, y: 290 },
                                end: { x: 1000, y: 290 }
                            },
                        ],
                        color: ARROW_COLOR,
                        info: qk_dot_product_info,
                    },
                    {
                        segments: [
                            {
                                start: { x: 750, y: 440 },
                                end: { x: 900, y: 440 }
                            },
                            {
                                start: { x: 900, y: 440 },
                                end: { x: 900, y: 290 }
                            },
                            {
                                start: { x: 900, y: 290 },
                                end: { x: 1000, y: 290 }
                            },
                        ],
                        color: ARROW_COLOR,
                        info: qk_dot_product_info,
                    },

                    // self attention output
                    // x: 220 + 6 * MIN_SEP_HORIZONTAL,
                    // y: 290, 
                    {
                        segments: [
                            {
                                start: { x: 1575, y: 290 },
                                end: { x: 1660, y: 290 }
                            },
                            {
                                start: { x: 1660, y: 290 },
                                end: { x: 1660, y: 430 }
                            },
                            {
                                start: { x: 1660, y: 430 },
                                end: { x: 1750, y: 430 }
                            },
                        ],
                        color: ARROW_COLOR,
                        info: sa_dot_product_info,
                    },
                    {
                        segments: [
                            {
                                start: { x: 750, y: 740 },
                                end: { x: 1660, y: 740 }
                            },
                            {
                                start: { x: 1660, y: 740 },
                                end: { x: 1660, y: 430 }
                            },
                            {
                                start: { x: 1660, y: 430 },
                                end: { x: 1750, y: 430 }
                            },
                        ],
                        color: ARROW_COLOR,
                        info: sa_dot_product_info,
                    },
                    {
                        segments: [
                            {
                                start: { x: 70, y: 340 },
                                end: { x: 70, y: -70 }
                            },
                            {
                                start: { x: 70, y: -70 },
                                end: { x: 2150, y: -70 }
                            },
                            {
                                start: { x: 2150, y: -70 },
                                end: { x: 2150, y: 405 }
                            },
                        ],
                        color: "#333",
                        info: residual_connection_info,
                    },
                    {
                        segments: [
                            // {
                            //     start: { x: 70, y: 340 },
                            //     end: { x: 70, y: -70 }
                            // },
                            // {
                            //     start: { x: 70, y: -70 },
                            //     end: { x: 2150, y: -70 }
                            // },
                            // {
                            //     start: { x: 2150, y: -70 },
                            //     end: { x: 2150, y: 405 }
                            // },
                            {
                                start: { x: 2175, y: 420 },
                                end: { x: 2200, y: 420 },
                            },
                            {
                                start: { x: 2200, y: 420 },
                                end: { x: 2200, y: -70 },
                            },
                            {
                                start: { x: 2200, y: -70 },
                                end: { x: 2700, y: -70 },
                            },
                            {
                                start: { x: 2700, y: -70 },
                                end: { x: 2700, y: 400 },
                            }
                        ],
                        color: "#333",
                        info: residual_connection_info,
                    },
                ],



            },
            // TODO make root again
            'root': {
                boxes: [
                    {
                        x: 50, y: 130, text: '', width: SINGLE_VECTOR_WIDTH, height: SINGLE_VECTOR_HEIGHT * 2,
                        info: x_raw_info, link: x_raw_link, opacity: 0.4
                    },
                    {
                        x: 50 + MIN_SEP_HORIZONTAL, y: 130, text: '', width: SINGLE_VECTOR_WIDTH, height: SINGLE_VECTOR_HEIGHT,
                        info: x_tok_info, link: x_tok_link, opacity: 0.4
                    },
                    {
                        x: 50 + 2 * MIN_SEP_HORIZONTAL, y: 130, text: '', width: SINGLE_VECTOR_WIDTH * 4, height: SINGLE_VECTOR_HEIGHT,
                        info: x_embd_info, link: x_embd_link, opacity: 0.4
                    },
                    {
                        x: 50 + 3 * MIN_SEP_HORIZONTAL - (SINGLE_LATEX_CHAR_WIDTH / 2),
                        y: 130 + (SINGLE_VECTOR_HEIGHT / 2) - (SINGLE_LATEX_CHAR_HEIGHT / 2),
                        text: '', width: SINGLE_LATEX_CHAR_WIDTH, height: SINGLE_LATEX_CHAR_HEIGHT,
                        info: addition_info,
                        opacity: 0.2
                    },
                    {
                        x: 50 + 3 * MIN_SEP_HORIZONTAL + 150,
                        y: 130 + (SINGLE_VECTOR_HEIGHT / 2) - (50),
                        text: 'Succesive applications of self-attention blocks',
                        width: 350, height: 100,
                        info: attn_info,
                        link: attn_link,
                        opacity: 1
                    },
                    {
                        x: 50 + 4 * MIN_SEP_HORIZONTAL + 450,
                        y: 130 + (SINGLE_VECTOR_HEIGHT / 2) - (50),
                        text: 'Logits',
                        width: 100, height: 100,
                        opacity: 0.4,
                        info: logits_info
                    },
                    {
                        x: 50 + 5 * MIN_SEP_HORIZONTAL + 450,
                        y: 130 + (SINGLE_VECTOR_HEIGHT / 2) - (50),
                        text: 'Output probabilities for next token',
                        width: 280, height: 100,
                        opacity: 0.4,
                        info: est_prob_dist_info
                    },
                    {
                        x: 50 + 5 * MIN_SEP_HORIZONTAL + 830,
                        y: 130 + (SINGLE_VECTOR_HEIGHT / 2) - (SINGLE_LATEX_CHAR_HEIGHT / 2) - 10,
                        text: '', width: SINGLE_LATEX_CHAR_WIDTH + 10, height: SINGLE_LATEX_CHAR_HEIGHT + 10,
                        info: x_tok_t_plus_1_info,
                        opacity: 0.4
                    },
                ],
                latexLabels: [
                    {
                        x: 50 + (SINGLE_VECTOR_WIDTH / 2), y: 100,
                        text: 'x_{raw}',
                        color: LATEX_LABEL_COLOR, width: SINGLE_LATEX_CHAR_WIDTH, height: SINGLE_LATEX_CHAR_HEIGHT,
                        info: x_raw_info, link: x_raw_link
                    },
                    {
                        x: 50 + MIN_SEP_HORIZONTAL + (SINGLE_VECTOR_WIDTH / 2), y: 100,
                        text: 'x_{tok}',
                        color: LATEX_LABEL_COLOR, width: SINGLE_LATEX_CHAR_WIDTH, height: SINGLE_LATEX_CHAR_HEIGHT,
                        info: x_tok_info, link: x_tok_link
                    },
                    {
                        x: 50 + 2 * MIN_SEP_HORIZONTAL + (SINGLE_VECTOR_WIDTH * 2), y: 100,
                        text: 'x_{embd}',
                        color: LATEX_LABEL_COLOR, width: SINGLE_LATEX_CHAR_WIDTH, height: SINGLE_LATEX_CHAR_HEIGHT,
                        info: x_embd_info, link: x_embd_link
                    },
                    {
                        x: 50 + 3 * MIN_SEP_HORIZONTAL, y: 130 + (SINGLE_VECTOR_HEIGHT / 2) - 8,
                        text: '\\boldsymbol{+}',
                        width: SINGLE_LATEX_CHAR_WIDTH, height: SINGLE_LATEX_CHAR_HEIGHT, info: addition_info
                    },
                    {
                        x: 50 + 3 * MIN_SEP_HORIZONTAL + 985,
                        y: 250,
                        text: '\\hat{P}( x_{tok}^{(t+1)} | x_{tok}^{(...t)} )',
                        color: LATEX_LABEL_COLOR, width: 4 * SINGLE_LATEX_CHAR_WIDTH, height: SINGLE_LATEX_CHAR_HEIGHT,
                        info: est_prob_dist_info
                    },
                    {
                        x: 50 + 3 * MIN_SEP_HORIZONTAL + 1262,
                        y: 130 + (SINGLE_VECTOR_HEIGHT / 2) - 12,
                        text: 'x_{tok}^{(t+1)}',
                        color: LATEX_LABEL_COLOR, width: SINGLE_LATEX_CHAR_WIDTH, height: SINGLE_LATEX_CHAR_HEIGHT,
                        info: x_tok_t_plus_1_info
                    },
                    // { x: 450, y: 250, text: '\\hat{y}', color: '#ffffff', info: "Output prediction", link: 'box1' }
                ],
                textLabels: [
                    {
                        x: 50 + SINGLE_VECTOR_WIDTH + 40,
                        y: 130 + (SINGLE_VECTOR_HEIGHT / 2) - 10,
                        text: 'Tokenization',
                        color: LATEX_LABEL_COLOR,
                        info: tokenization_info,
                        link: tokenization_link
                    },
                    {
                        x: 50 + MIN_SEP_HORIZONTAL + SINGLE_VECTOR_WIDTH + 20,
                        y: 130 + (SINGLE_VECTOR_HEIGHT / 2) - 10,
                        text: 'Token Embedding',
                        color: LATEX_LABEL_COLOR,
                        info: token_embd_info,
                        link: token_embd_link
                    },
                    {
                        x: 50 + 3 * MIN_SEP_HORIZONTAL - SINGLE_LATEX_CHAR_WIDTH - 20,
                        y: 130 + (SINGLE_VECTOR_HEIGHT / 2) + 260,
                        text: 'Positional Encoding',
                        color: "#555", //LATEX_LABEL_COLOR,
                        info: pos_enc_info,
                        link: pos_enc_link
                    },
                    {
                        x: 50 + 3 * MIN_SEP_HORIZONTAL + 50,
                        y: 130 + (SINGLE_VECTOR_HEIGHT / 2) - 10,
                        text: '(Dropout)',
                        color: '#444',
                        info: dropout_info
                    },
                    {
                        x: 50 + 3 * MIN_SEP_HORIZONTAL + 512,
                        y: 130 + (SINGLE_VECTOR_HEIGHT / 2) - 10,
                        text: 'Linear Projection',
                        color: LATEX_LABEL_COLOR,
                        info: lin_prj_info
                    },
                    {
                        x: 50 + 3 * MIN_SEP_HORIZONTAL + 770,
                        y: 130 + (SINGLE_VECTOR_HEIGHT / 2) - 10,
                        text: 'Softmax',
                        color: LATEX_LABEL_COLOR,
                        info: softmax_info

                    },
                    {
                        x: 50 + 3 * MIN_SEP_HORIZONTAL + 1145,
                        y: 130 + (SINGLE_VECTOR_HEIGHT / 2) - 10,
                        text: 'Sampling',
                        color: LATEX_LABEL_COLOR,
                        info: sampling_info

                    },
                ],
                arrows: [
                    {
                        start: { x: 50 + SINGLE_VECTOR_WIDTH, y: 130 + (SINGLE_VECTOR_HEIGHT / 2) },
                        end: { x: 50 + MIN_SEP_HORIZONTAL, y: 130 + (SINGLE_VECTOR_HEIGHT / 2) },
                        color: ARROW_COLOR,
                        info: tokenization_info,
                        link: tokenization_link
                    },
                    {
                        start: { x: 50 + MIN_SEP_HORIZONTAL + SINGLE_VECTOR_WIDTH, y: 130 + (SINGLE_VECTOR_HEIGHT / 2) },
                        end: { x: 50 + 2 * MIN_SEP_HORIZONTAL, y: 130 + (SINGLE_VECTOR_HEIGHT / 2) },
                        color: ARROW_COLOR,
                        info: token_embd_info,
                        link: token_embd_link
                    },
                    {
                        start: { x: 50 + 2 * MIN_SEP_HORIZONTAL + 4 * SINGLE_VECTOR_WIDTH, y: 130 + (SINGLE_VECTOR_HEIGHT / 2) },
                        end: { x: 50 + 3 * MIN_SEP_HORIZONTAL - SINGLE_LATEX_CHAR_WIDTH / 2, y: 130 + (SINGLE_VECTOR_HEIGHT / 2) },
                        color: ARROW_COLOR, info: pos_encoding_arrows_info,
                    },
                    {
                        start: { x: 50 + 3 * MIN_SEP_HORIZONTAL, y: 130 + (SINGLE_VECTOR_HEIGHT / 2) + 170 },
                        end: { x: 50 + 3 * MIN_SEP_HORIZONTAL, y: 130 + (SINGLE_VECTOR_HEIGHT / 2) + SINGLE_LATEX_CHAR_HEIGHT / 2 },
                        color: "#333",
                        // color: ARROW_COLOR, 
                        info: pos_encoding_arrows_info,
                    },
                    {
                        start: { x: 50 + 3 * MIN_SEP_HORIZONTAL + 25, y: 130 + (SINGLE_VECTOR_HEIGHT / 2) },
                        end: { x: 50 + 3 * MIN_SEP_HORIZONTAL + 150, y: 130 + (SINGLE_VECTOR_HEIGHT / 2) },
                        color: ARROW_COLOR,
                        info: dropout_info
                    },
                    {
                        start: { x: 50 + 3 * MIN_SEP_HORIZONTAL + 500, y: 130 + (SINGLE_VECTOR_HEIGHT / 2) },
                        end: { x: 50 + 3 * MIN_SEP_HORIZONTAL + 650, y: 130 + (SINGLE_VECTOR_HEIGHT / 2) },
                        color: ARROW_COLOR,
                        info: lin_prj_info
                    },
                    {
                        start: { x: 50 + 3 * MIN_SEP_HORIZONTAL + 750, y: 130 + (SINGLE_VECTOR_HEIGHT / 2) },
                        end: { x: 50 + 3 * MIN_SEP_HORIZONTAL + 850, y: 130 + (SINGLE_VECTOR_HEIGHT / 2) },
                        color: ARROW_COLOR,
                        info: softmax_info
                    },
                    {
                        start: { x: 50 + 3 * MIN_SEP_HORIZONTAL + 1130, y: 130 + (SINGLE_VECTOR_HEIGHT / 2) },
                        end: { x: 50 + 3 * MIN_SEP_HORIZONTAL + 1230, y: 130 + (SINGLE_VECTOR_HEIGHT / 2) },
                        color: ARROW_COLOR,
                        info: sampling_info
                    },
                ],

                separators: [
                    {
                        x: 220,
                        // y1: 100,
                        // y2: 700,
                        type: 'dashed',
                        info: 'This seperates the tokenization process (left) from the transformer architecture (right)',
                        color: '#444'
                    },
                    {
                        x: 1790,
                        // y1: 100,
                        // y2: 700,
                        type: 'dashed',
                        info: 'This seperates the transformer architecture (left) from the token sampling process (right)',
                        color: '#444'
                    },
                ],

                paths: [
                    {
                        x: 50 + 3 * MIN_SEP_HORIZONTAL - SINGLE_LATEX_CHAR_WIDTH, y: 130 + (SINGLE_VECTOR_HEIGHT / 2) + 150,
                        width: 100, height: 100, pathD: positionalEncodingPath, color: POS_ENC_COLOR,
                        info: pos_enc_info,
                        link: pos_enc_link
                    }
                ],

            },
        };
        // END SVG Definition
        //-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=

        //-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
        // SVG Rendering and Event Handling
        //-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
        function drawContent(view) {
            contentGroup.selectAll('*').remove();

            if (svgContent[view]) {
                // Render boxes
                if (svgContent[view].boxes) {
                    svgContent[view].boxes.forEach(item => {
                        const rect = contentGroup.append('rect')
                            .attr('x', item.x)
                            .attr('y', item.y)
                            .attr('width', item.width)
                            .attr('height', item.height)
                            .attr('fill', RECT_FILL_COLOR)
                            .attr('stroke', RECT_STROKE_COLOR)
                            .attr('opacity', item.opacity !== undefined ? item.opacity : 1)
                            .attr('data-info', item.info)
                            .attr('data-link', item.link);

                        contentGroup.append('text')
                            .attr('x', item.x + item.width / 2)
                            .attr('y', item.y + item.height / 2)
                            .attr('text-anchor', 'middle')
                            .attr('dominant-baseline', 'central')
                            .attr('fill', TEXT_COLOR)
                            .text(item.text)
                            .attr('data-info', item.info)
                            .attr('data-link', item.link);
                    });
                }


                // Render triangles
                if (svgContent[view].triangles) {
                    svgContent[view].triangles.forEach(triangle => {
                        drawTriangle(
                            triangle.x1, triangle.y1,
                            triangle.x2, triangle.y2,
                            triangle.x3, triangle.y3,
                            triangle.color,
                            triangle.info,
                            triangle.link,
                            triangle.opacity
                        );
                    });
                }

                // Render directed arrow connections
                if (svgContent[view].arrows) {
                    svgContent[view].arrows.forEach(arrow => {
                        const arrowElement = drawArrow(arrow.start, arrow.end, arrow.color);
                        arrowElement.attr('data-info', arrow.info)
                            .attr('data-link', arrow.link);
                    });
                }

                // Render anchoredArrows
                if (svgContent[view].anchoredArrows) {
                    svgContent[view].anchoredArrows.forEach(arrow => {
                        const arrowElement = drawAnchoredArrow(arrow.segments, arrow.color);
                        arrowElement.attr('data-info', arrow.info)
                            .attr('data-link', arrow.link);
                    });
                }

                // Render separators
                if (svgContent[view].separators) {
                    svgContent[view].separators.forEach(separator => {
                        drawSeparator(separator);
                    });
                }

                // Render LaTeX labels
                if (svgContent[view].latexLabels) {
                    svgContent[view].latexLabels.forEach(label => {
                        contentGroup.append(() => createLatexLabel(
                            label.x,
                            label.y,
                            label.text,
                            label.color,
                            label.info,
                            label.link,
                            label.width,
                            label.height
                        ));
                    });
                }

                // Render text labels
                if (svgContent[view].textLabels) {
                    svgContent[view].textLabels.forEach(label => {
                        contentGroup.append(() => createTextLabel(
                            label.x,
                            label.y,
                            label.text,
                            label.color,
                            label.info,
                            label.link
                        ));
                    });
                }

                // Render custom paths
                if (svgContent[view].paths) {
                    svgContent[view].paths.forEach(path => {
                        contentGroup.append('path')
                            .attr('d', path.pathD)
                            .attr('transform', `translate(${path.x}, ${path.y}) scale(${path.width / 30}, ${path.height / 30})`)
                            .attr('fill', 'none')
                            .attr('stroke', path.color)
                            .attr('stroke-width', 2)
                            .attr('class', 'custom-path')
                            .attr('data-info', path.info)
                            .attr('data-link', path.link)
                    });
                }

                attachEventListenersForView(view);
            } else {
                logMessage(`No content found for view: ${view}`);
            }
        }


        function attachEventListenersForView(view) {
            // Add listeners to boxes
            if (svgContent[view].boxes) {
                svgContent[view].boxes.forEach(item => {
                    const rect = contentLayer.select(`rect[x="${item.x}"][y="${item.y}"]`);
                    const text = contentLayer.select(`text[x="${item.x + item.width / 2}"][y="${item.y + item.height / 2}"]`);

                    addDefaultEventListeners(rect.node(), item.link);
                    addDefaultEventListeners(text.node(), item.link);

                    if (item.link) {
                        const originalRectStroke = rect.attr('stroke');
                        const originalTextFill = text.attr('fill');

                        rect.on('mouseover', function () { rect.attr('stroke', LINK_COLOR); })
                            .on('mouseout', function () { rect.attr('stroke', originalRectStroke); });

                        text.on('mouseover', function () { rect.attr('stroke', LINK_COLOR); })
                            .on('mouseout', function () { rect.attr('stroke', originalTextFill); });
                    }
                });
            }

            // Add listeners to triangles
            if (svgContent[view].triangles) {
                contentLayer.selectAll('.triangle').each(function () {
                    const triangle = d3.select(this);
                    const link = triangle.attr('data-link');
                    addDefaultEventListeners(triangle.node(), link);
                    if (link) {
                        const originalFill = triangle.attr('fill');
                        triangle.on('mouseover', function () {
                            triangle.attr('fill', LINK_COLOR);
                        })
                            .on('mouseout', function () {
                                triangle.attr('fill', originalFill);
                            });
                    }
                });
            }

            // Add listeners to latex labels
            if (svgContent[view].latexLabels) {
                contentLayer.selectAll('.latex-label').each(function () {
                    const label = d3.select(this);
                    const link = label.attr('data-link');
                    addDefaultEventListeners(this, link);
                    if (link) {
                        const originalColor = label.select('div').style('color');
                        label.on('mouseover', function () { label.select('div').style('color', LINK_COLOR); })
                            .on('mouseout', function () { label.select('div').style('color', originalColor); });
                    }
                });
            }

            // Add listeners to text labels
            if (svgContent[view].textLabels) {
                contentLayer.selectAll('.text-label').each(function () {
                    const label = d3.select(this);
                    const link = label.attr('data-link');
                    addDefaultEventListeners(this, link);
                    if (link) {
                        const originalFill = label.attr('fill');
                        label.on('mouseover', function () {
                            label.attr('fill', LINK_COLOR);
                            showInfo({ currentTarget: this });
                        })
                            .on('mouseout', function () {
                                label.attr('fill', originalFill);
                                clearInfo();
                            })
                    }
                });
            }

            // Add listeners to arrows
            if (svgContent[view].arrows) {
                contentLayer.selectAll('.arrow').each(function () {
                    const arrow = d3.select(this);
                    const link = arrow.attr('data-link');
                    addDefaultEventListeners(arrow.node(), link);
                    if (link) {
                        const originalColor = arrow.attr('stroke');
                        arrow.on('mouseover', function () {
                            arrow.attr('stroke', LINK_COLOR).attr('fill', LINK_COLOR);
                        })
                            .on('mouseout', function () {
                                arrow.attr('stroke', originalColor).attr('fill', originalColor);
                            });
                    }
                });
            }

            if (svgContent[view].anchoredArrows) {
                contentLayer.selectAll('.anchored-arrow').each(function () {
                    const arrow = d3.select(this);
                    const link = arrow.attr('data-link');
                    addDefaultEventListeners(arrow.node(), link);
                    if (link) {
                        const originalColor = arrow.attr('stroke');
                        arrow.on('mouseover', function () {
                            arrow.attr('stroke', LINK_COLOR).attr('fill', LINK_COLOR);
                        })
                            .on('mouseout', function () {
                                arrow.attr('stroke', originalColor).attr('fill', originalColor);
                            });
                    }
                });
            }

            // Add listeners to separators
            if (svgContent[view].separators) {
                contentLayer.selectAll('line').each(function () {
                    const separator = d3.select(this);
                    addDefaultEventListeners(separator.node());
                });
            }

            // Add listeners to custom paths
            if (svgContent[view].paths) {
                contentLayer.selectAll('.custom-path').each(function () {
                    const path = d3.select(this);
                    const link = path.attr('data-link');
                    addDefaultEventListeners(path.node(), link);
                    if (link) {
                        const originalStroke = path.attr('stroke');
                        path.on('mouseover', function () { path.attr('stroke', LINK_COLOR); })
                            .on('mouseout', function () { path.attr('stroke', originalStroke); });
                    }
                });
            }

            // Add listeners to navigation buttons
            navigationLayer.selectAll('.nav-button').each(function () {
                addDefaultEventListeners(this);
            });
        }


        function addDefaultEventListeners(element, link) {
            element.addEventListener('mouseover', showInfo);
            element.addEventListener('mouseout', clearInfo);
            element.addEventListener('click', function (event) {
                if (link) {
                    navigateTo(link);
                }
                // const infoElement = document.getElementById('info');
                // navigator.clipboard.writeText(infoElement.textContent).then(function () {
                //     logMessage("Text has been copied to clipboard");
                // }).catch(function (err) {
                //     logMessage("Failed to copy text: ", err);
                // });
                clearInfo(event);
            });

            // Keyboard events
            element.addEventListener('focus', showInfo);
            element.addEventListener('blur', clearInfo);

            // Update cursor style
            element.style.cursor = link ? 'pointer' : 'default';
        }


        function drawTriangle(x1, y1, x2, y2, x3, y3, color, info, link, opacity = 1) {
            const points = [
                [x1, y1],
                [x2, y2],
                [x3, y3]
            ];

            const triangle = contentGroup.append('polygon')
                .attr('points', points.map(p => p.join(',')).join(' '))
                .attr('fill', color)
                .attr('stroke', RECT_STROKE_COLOR)
                .attr('class', 'triangle')
                .attr('data-info', info)
                .attr('data-link', link)
                .attr('opacity', opacity);

            return triangle;
        }


        function drawArrow(start, end, color) {
            const arrowSize = 10;
            const angle = Math.atan2(end.y - start.y, end.x - start.x);

            // Calculate the position of the arrowhead
            const arrowX = end.x - arrowSize * Math.cos(angle);
            const arrowY = end.y - arrowSize * Math.sin(angle);

            // Create a group for the arrow
            const arrowGroup = contentGroup.append('g')
                .attr('class', 'arrow')
                .attr('stroke', color)
                .attr('fill', color);

            // Draw the line
            arrowGroup.append('line')
                .attr('x1', start.x)
                .attr('y1', start.y)
                .attr('x2', end.x)
                .attr('y2', end.y)
                .attr('stroke-width', 2);

            // Draw the arrowhead
            arrowGroup.append('path')
                .attr('d', `M${end.x},${end.y} L${end.x - arrowSize * Math.cos(angle - Math.PI / 6)},${end.y - arrowSize * Math.sin(angle - Math.PI / 6)} L${end.x - arrowSize * Math.cos(angle + Math.PI / 6)},${end.y - arrowSize * Math.sin(angle + Math.PI / 6)} Z`);

            // arrowGroup.append('path')
            //     .attr('d', `M${end.x},${end.y} L${arrowX - arrowSize * Math.cos(angle - Math.PI / 6)},${arrowY - arrowSize * Math.sin(angle - Math.PI / 6)} L${arrowX - arrowSize * Math.cos(angle + Math.PI / 6)},${arrowY - arrowSize * Math.sin(angle + Math.PI / 6)} Z`);

            return arrowGroup;
        }

        function drawAnchoredArrow(segments, color) {
            const arrowSize = 10;
            const path = d3.path();

            segments.forEach((segment, index) => {
                if (index === 0) {
                    path.moveTo(segment.start.x, segment.start.y);
                }
                path.lineTo(segment.end.x, segment.end.y);
            });

            const lastSegment = segments[segments.length - 1];
            const angle = Math.atan2(lastSegment.end.y - lastSegment.start.y, lastSegment.end.x - lastSegment.start.x);

            const arrowGroup = contentGroup.append('g')
                .attr('class', 'anchored-arrow')
                .attr('stroke', color)
                .attr('fill', color);

            arrowGroup.append('path')
                .attr('d', path.toString())
                .attr('fill', 'none')
                .attr('stroke-width', 2);

            arrowGroup.append('path')
                .attr('d', `M${lastSegment.end.x},${lastSegment.end.y} L${lastSegment.end.x - arrowSize * Math.cos(angle - Math.PI / 6)},${lastSegment.end.y - arrowSize * Math.sin(angle - Math.PI / 6)} L${lastSegment.end.x - arrowSize * Math.cos(angle + Math.PI / 6)},${lastSegment.end.y - arrowSize * Math.sin(angle + Math.PI / 6)} Z`);

            return arrowGroup;
        }



        function drawSeparator(separator) {
            const line = contentGroup.append('line')
                .attr('stroke', separator.color || '#888')
                .attr('stroke-width', 1);

            if (separator.x !== undefined) {
                // Vertical line
                line.attr('x1', separator.x)
                    .attr('x2', separator.x)
                    .attr('y1', separator.y1 || -SINGLE_VECTOR_HEIGHT)
                    .attr('y2', separator.y2 || svgHeight + SINGLE_VECTOR_HEIGHT);
            } else if (separator.y !== undefined) {
                // Horizontal line
                line.attr('y1', separator.y)
                    .attr('y2', separator.y)
                    .attr('x1', separator.x1 || -100)
                    .attr('x2', separator.x2 || 2 * currentSvgWidth);
            }

            if (separator.type === 'dashed') {
                line.attr('stroke-dasharray', '5,5');
            } else if (separator.type === 'dotted') {
                line.attr('stroke-dasharray', '2,2');
            }

            line.attr('data-info', separator.info || "Separator");
        }

        // function adjustSVGWidth() {
        //     const screenWidth = window.innerWidth;
        //     const newWidth = Math.floor(screenWidth * 0.68);

        //     const svgContainer = document.getElementById('svg-container');
        //     const svg = document.getElementById('svg');

        //     svgContainer.style.width = `${newWidth}px`;
        //     svg.setAttribute('width', newWidth);
        //     logMessage(newWidth)

        //     // Adjust other elements if necessary
        //     const rightContainer = document.getElementById('right-container');
        //     rightContainer.style.width = `${screenWidth - newWidth}px`;
        // }
        function adjustSVGWidth() {
            const screenWidth = window.innerWidth;
            const leftWidth = Math.floor(screenWidth * 0.7);
            const paddingBetween = 30;
            const rightWidth = screenWidth - leftWidth - paddingBetween;

            const svgContainer = document.getElementById('svg-container');
            const svg = document.getElementById('svg');
            const rightContainer = document.getElementById('right-container');

            svgContainer.style.width = `${leftWidth}px`;
            svg.setAttribute('width', leftWidth);

            rightContainer.style.width = `${rightWidth}px`;

            // Adjust the info and references containers
            const infoContainer = document.getElementById('info-container');
            const referencesContainer = document.getElementById('references-container');

            infoContainer.style.width = `${rightWidth - 20}px`; // 20px for padding
            referencesContainer.style.width = `${rightWidth - 20}px`; // 20px for padding

            // Store the new SVG width globally
            window.currentSvgWidth = leftWidth;
        }


        // END SVG Rendering and Event Handling
        //-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=

        //-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
        // SVG Zoom
        //-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
        let lastPixelRatio = window.devicePixelRatio;

        function checkZoom() {
            const newPixelRatio = window.devicePixelRatio;
            if (newPixelRatio !== lastPixelRatio) {
                lastPixelRatio = newPixelRatio;
                adjustSVGWidth();
            }
            requestAnimationFrame(checkZoom);
        }

        function setupZoom() {
            zoom = d3.zoom()
                .scaleExtent([0.25, 2])
                .on('zoom', zoomed);

            const svg = d3.select("#svg")
                .call(zoom)
                .on("dblclick.zoom", null);

            contentGroup = contentLayer
                .append("g")
                .attr("class", "content-group");

            // Store the initial transform
            // zoom.transform(svg, d3.zoomIdentity);

            // Apply initial zoom and pan
            // applyZoomAndPan(0.7, 100, 50);
        }

        function zoomed(event) {
            contentGroup.attr('transform', event.transform);
            // logMessage(`Zoom: ${event.transform.k.toFixed(2)}, Pan: (${event.transform.x.toFixed(2)}, ${event.transform.y.toFixed(2)})`);
        }

        function applyZoomAndPan(scale, x, y) {
            const svg = d3.select("#svg");
            const transform = d3.zoomIdentity.translate(x, y).scale(scale);
            svg.call(zoom.transform, transform);
        }


        function resetZoom() {
            svg.transition().duration(750).call(
                zoom.transform,
                d3.zoomIdentity.translate(10, 150).scale(0.60)
                // d3.zoomIdentity,
                // d3.zoomTransform(svg.node()).invert([currentSvgWidth / 2, svgHeight / 2])
            );
        }
        // END SVG Zoom
        //-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=

        //-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
        // SVG Navigation
        //-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
        function drawNavigation() {
            navigationLayer.selectAll('.nav-button').remove();
            drawArrowButton('back-button', 5, 5, arrowPaths.back, navigateBack, undoNavigationHistory.length > 0);
            drawArrowButton('forward-button', 55, 5, arrowPaths.forward, navigateForward, redoNavigationHistory.length > 0);
            drawArrowButton('reset-button', 105, 5, arrowPaths.reset, resetZoom, true);
            // drawArrowButton('reset-button', currentSvgWidth - 100, 5, arrowPaths.reset, resetZoom, true);
        }


        function drawArrowButton(id, x, y, pathD, clickHandler, isEnabled) {
            const group = navigationLayer.append('g')
                .attr('class', 'nav-button')
                .attr('id', id)
                .attr('transform', `translate(${x}, ${y})`)
                .style('cursor', isEnabled ? 'pointer' : 'default');

            // Button background
            group.append('rect')
                .attr('width', 40)
                .attr('height', 40)
                .attr('rx', 5)
                .attr('ry', 5)
                .attr('fill', isEnabled ? BUTTON_FILL_COLOR : DISABLED_BUTTON_FILL_COLOR)
                .attr('stroke', isEnabled ? BUTTON_STROKE_COLOR : DISABLED_BUTTON_STROKE_COLOR);

            // Arrow
            group.append('path')
                .attr('d', pathD)
                .attr('fill', 'none')
                .attr('stroke', isEnabled ? BUTTON_ARROW_COLOR : DISABLED_BUTTON_ARROW_COLOR)
                .attr('stroke-width', 2)
                .attr('stroke-linecap', 'round')
                .attr('stroke-linejoin', 'round')
                .attr('transform', 'translate(5, 5)');

            if (isEnabled) {
                group.on('click', clickHandler);

                // Hover effect
                group.on('mouseover', function () {
                    d3.select(this).select('rect').attr('fill', BUTTON_HOVER_FILL_COLOR);
                    d3.select(this).select('path').attr('stroke', BUTTON_HOVER_ARROW_COLOR);
                });

                group.on('mouseout', function () {
                    d3.select(this).select('rect').attr('fill', BUTTON_FILL_COLOR);
                    d3.select(this).select('path').attr('stroke', BUTTON_ARROW_COLOR);
                });
            }
        }


        function navigateTo(view) {
            if (currentView == view) {
                logMessage("repeat navigateTo. exiting");
                return;
            }
            undoNavigationHistory.push(currentView);
            redoNavigationHistory = []; // Clear redo history
            currentView = view;
            updateView();
            resetZoom();
        }

        function navigateBack() {
            if (undoNavigationHistory.length > 0) {
                redoNavigationHistory.push(currentView);
                currentView = undoNavigationHistory.pop();
                updateView();
                resetZoom();
            }
        }

        function navigateForward() {
            if (redoNavigationHistory.length > 0) {
                undoNavigationHistory.push(currentView);
                currentView = redoNavigationHistory.pop();
                updateView();
                resetZoom();
            }
        }

        function updateView() {
            drawNavigation();
            drawContent(currentView);
        }
        // END SVG Navigation
        //-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=

        //-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
        // LaTeX and plaintext labels
        //-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
        function createLatexLabel(x, y, text, color, labelInfo, link, width = 250, height = 40) {
            const foreignObject = document.createElementNS('http://www.w3.org/2000/svg', 'foreignObject');
            foreignObject.setAttribute('x', x - (width / 2));
            foreignObject.setAttribute('y', y - 20);
            foreignObject.setAttribute('width', width);
            foreignObject.setAttribute('height', height);
            foreignObject.classList.add('latex-label');
            foreignObject.dataset.latex = text;
            foreignObject.dataset.info = labelInfo;

            if (link) {
                foreignObject.dataset.link = link;
                // foreignObject.style.cursor = 'pointer';
            }

            const div = document.createElement('div');
            div.style.fontSize = LATEX_FONT_SIZE + 'px';
            div.style.color = color;
            div.style.display = 'flex';
            div.style.justifyContent = 'center';
            div.style.alignItems = 'center';
            div.style.height = '100%';

            katex.render(text, div, {
                throwOnError: true
            });

            foreignObject.appendChild(div);

            return foreignObject;
        }


        function createTextLabel(x, y, text, color, labelInfo, link) {
            const textElement = document.createElementNS('http://www.w3.org/2000/svg', 'text');
            textElement.setAttribute('x', x);
            textElement.setAttribute('y', y);
            textElement.setAttribute('fill', color);
            textElement.textContent = text;
            textElement.classList.add('text-label');
            textElement.dataset.info = labelInfo;
            // textElement.setAttribute('data-info', labelInfo);
            if (link) {
                textElement.dataset.link = link;
                // textElement.setAttribute('data-link', link);
            }
            return textElement;
        }
        // END LaTeX and plaintext labels
        //-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=

        //-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
        // Display detailed info top-right
        //-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
        function showInfo(event) {
            const target = event.currentTarget;
            let content = '';

            if (target.tagName === 'A' && target.parentElement.parentElement.id === 'references-list') {
                const key = target.textContent.split(" (")[0]
                const ref = references.find(r => r.title === key);
                if (ref) {
                    content = formatReferenceInfo(ref);
                }
            } else if (target.classList.contains('custom-path')) {
                content = target.getAttribute('data-info') || "Custom path";
            } else if (target.classList.contains('arrow') || target.classList.contains('anchored-arrow')) {
                content = target.getAttribute('data-info') || "Arrow connection";
            } else if (target.classList.contains('nav-button')) {
                if (target.id === 'back-button') {
                    content = "Navigate back to the previous view.";
                } else if (target.id === 'forward-button') {
                    content = "Navigate forward to the next view.";
                } else if (target.id === 'reset-button') {
                    content = "Reset the zoom and pan states.";
                }
            } else if (target.classList.contains('latex-label')) {
                // content = target.dataset.info;
                content = target.getAttribute('data-info');
            } else if (target.classList.contains('text-label')) {
                content = target.getAttribute('data-info');
            } else if (target.tagName === 'rect' || target.tagName === 'text') {
                content = target.getAttribute('data-info');
            } else if (target.classList.contains('triangle')) {
                content = target.getAttribute('data-info') || "Triangle";
            } else if (target.tagName === 'line') {
                content = target.getAttribute('data-info') || "Separator";
            }
            renderInfoContent(document.getElementById('info'), content);
        }


        function renderInfoContent(element, content) {
            element.innerHTML = '';
            let currentIndex = 0;

            while (currentIndex < content.length) {
                if (content.startsWith('$$', currentIndex)) {
                    // LaTeX content
                    const endIndex = content.indexOf('$$', currentIndex + 2);
                    if (endIndex === -1) {
                        console.error('Unclosed LaTeX at position', currentIndex);
                        break;
                    }
                    const latexContent = content.slice(currentIndex + 2, endIndex);
                    const span = document.createElement('span');
                    katex.render(latexContent, span, {
                        throwOnError: false,
                        displayMode: false
                    });
                    element.appendChild(span);
                    currentIndex = endIndex + 2;
                } else if (content.startsWith('**', currentIndex)) {
                    // Bold content
                    const endIndex = content.indexOf('**', currentIndex + 2);
                    if (endIndex === -1) {
                        console.error('Unclosed bold at position', currentIndex);
                        break;
                    }
                    const boldContent = content.slice(currentIndex + 2, endIndex);
                    const strong = document.createElement('strong');
                    strong.textContent = boldContent;
                    element.appendChild(strong);
                    currentIndex = endIndex + 2;
                } else {
                    // Plain text
                    const nextSpecialChar = Math.min(
                        content.indexOf('$$', currentIndex) === -1 ? Infinity : content.indexOf('$$', currentIndex),
                        content.indexOf('**', currentIndex) === -1 ? Infinity : content.indexOf('**', currentIndex)
                    );
                    const textContent = content.slice(currentIndex, nextSpecialChar === Infinity ? undefined : nextSpecialChar);
                    const lines = textContent.split('\n');
                    lines.forEach((line, index) => {
                        element.appendChild(document.createTextNode(line));
                        if (index < lines.length - 1) {
                            element.appendChild(document.createElement('br'));
                        }
                    });
                    currentIndex = nextSpecialChar === Infinity ? content.length : nextSpecialChar;
                }
            }
        }

        function clearInfo(event) {
            const defaultStr = "";
            document.getElementById('info').textContent = defaultStr;
        }
        // END Display detailed info top-right
        //-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=

        //-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
        // Display references bot right
        //-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
        function formatReferenceInfo(ref) {
            let content = "";
            content += `**Reference Name:** ${ref.title}\n\n`;
            content += `**Description:** ${ref.info}\n\n`
            content += '**Authors:**\n';
            ref.authors.forEach(author => {
                content += `â¢ ${author}\n`;
            });
            content += `\n**Link type:** ${ref.refType}\n\n`;
            return content;
        }

        function renderReferences() {
            const referencesList = document.getElementById('references-list');
            referencesList.innerHTML = '';

            if (references) {
                references.forEach(ref => {
                    const li = document.createElement('li');
                    const a = document.createElement('a');
                    a.href = ref.link;
                    a.target = "_blank";
                    a.textContent = ref.title + ` (${ref.refType})`;
                    a.addEventListener('mouseover', showInfo);
                    a.addEventListener('mouseout', clearInfo);
                    li.appendChild(a);
                    referencesList.appendChild(li);
                });
            }
        }

        // For debugging or general message to user
        function logMessage(message) {
            const logContainer = document.getElementById('log-container');
            logContainer.innerHTML += `<p>${message}</p>`;
            logContainer.scrollTop = logContainer.scrollHeight;
        }

        //-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
        // Main
        //-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
        window.onload = function () {
            adjustSVGWidth();
            setupZoom();
            updateView();
            renderReferences();
            checkZoom(); // Start checking for zoom changes
            // applyZoomAndPan(.60, 10, 150)

            // zoom in effect on load to show there is more and zooming is a thing
            // applyZoomAndPan(0.7, 100, 50);
            resetZoom();
        }

        let resizeTimer;
        window.addEventListener('resize', function () {
            clearTimeout(resizeTimer);
            resizeTimer = setTimeout(adjustSVGWidth, 100);
        });

    </script>
</body>